{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdeiZ3t1NLNJZgK0QoTZMl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvinashKushwah/cs6910_assignment1/blob/main/Question2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zRZiwucHB0e",
        "outputId": "1b0a7262-a3be-4d21-b82c-11d5cd485c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "(10000, 784)\n",
            "enter number of Hidden layers:4\n",
            "Enter number of neurons:128\n",
            "Enter number of neurons:256\n",
            "Enter number of neurons:256\n",
            "Enter number of neurons:64\n",
            "enter activation functions ReLU,sigmoid,tanh :ReLU\n",
            "Enter Index of an Image:8888\n",
            "[[4.23178736e-029]\n",
            " [1.00000000e+000]\n",
            " [5.33459058e-032]\n",
            " [9.77689517e-060]\n",
            " [2.03805960e-029]\n",
            " [3.68819306e-012]\n",
            " [6.98072064e-074]\n",
            " [6.59735366e-034]\n",
            " [6.63840655e-103]\n",
            " [1.08773261e-094]]\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "x_trainFlat = x_train.reshape(len(x_train),784).astype('float32')/255\n",
        "#print(x_trainFlat.shape)\n",
        "x_testFlat = x_test.reshape(len(x_test),784).astype('float32')/255\n",
        "print(x_trainFlat.shape[0])\n",
        "print(x_testFlat.shape)\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "def sigmoid(x):# limit the range of x to avoid overflow\n",
        "  return 1.0 / (1.0 + np.exp(-x))\n",
        " \n",
        "def sigmoid_diff(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def softmax(Z):\n",
        "  exp = np.exp(Z - np.max(Z))      #removing numerical instablity\n",
        "  return exp / exp.sum(axis=0)\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def ReLU_deriv(x):\n",
        "    return x > 0\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_deriv(z):\n",
        "    return 1 - np.square(np.tanh(z))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Initalizing Weights and Biases\n",
        "\n",
        "def Initalize_Wb(layers, numberOfNeurons):\n",
        "    W = []\n",
        "    b = []\n",
        "    for i in range(layers):\n",
        "        if i == 0:\n",
        "            # Input layer\n",
        "            w_temp = np.random.rand(numberOfNeurons[i], 784) - 0.5\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        else:\n",
        "            # Hidden layers\n",
        "            w_temp = np.random.rand(numberOfNeurons[i], numberOfNeurons[i-1]) - 0.5\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        W.append(w_temp)\n",
        "        b.append(b_temp)\n",
        "    # Output layer\n",
        "    output_w = np.random.rand(10, numberOfNeurons[-1]) - 0.5\n",
        "    output_b = np.zeros((10, 1))\n",
        "    W.append(output_w)\n",
        "    b.append(output_b)\n",
        "    return W, b\n",
        "\n",
        "\n",
        " \n",
        "    output_w = np.random.rand(10,numberOfNeurons[layers-1])-0.5\n",
        "    output_b = np.random.zeros((10,1))\n",
        "    W.append(output_w)\n",
        "    b.append(output_b)\n",
        "    return W,b\n",
        "\n",
        "\n",
        "\n",
        "#Feedforward Neural Network\n",
        "def feedForward(input_data,W,b,layers,activation):\n",
        "  \n",
        "   output =[]\n",
        "   temp =[]\n",
        "   A =[]\n",
        "  #  print(\"hello\",b[0].shape)\n",
        "  #  print(b[0].reshape(-1,1).shape)\n",
        "   Y =np.dot(W[0],input_data) + b[0]\n",
        "   temp.append(Y)\n",
        "   if(activation == 'ReLU'):\n",
        "      Y=ReLU(Y)\n",
        "   elif(activation == 'sigmoid'):\n",
        "      Y=sigmoid(Y)\n",
        "   else:\n",
        "      Y = tanh(Y)\n",
        "        \n",
        "   A.append(Y)\n",
        "   for i in range(1,layers):\n",
        "      Y =np.dot(W[i],Y) + b[i]\n",
        "      temp.append(Y)\n",
        "      if(activation == 'ReLU'):\n",
        "        Y=ReLU(Y)\n",
        "        A.append(Y)\n",
        "      elif(activation == 'sigmoid'):\n",
        "        Y=sigmoid(Y)\n",
        "        A.append(Y)\n",
        "      else:\n",
        "        Y = tanh(Y)\n",
        "        A.append(Y)\n",
        "\n",
        "   temp2=np.dot(W[layers],Y) + b[layers]\n",
        "   temp.append(temp2)\n",
        "   if(activation == 'ReLU'):\n",
        "     output=(softmax(ReLU(temp2)))\n",
        "   elif(activation == 'sigmoid'):\n",
        "     output=(softmax(sigmoid(temp2)))\n",
        "   else:\n",
        "     output=(softmax(tanh(temp2)))\n",
        "\n",
        "   #print(\"softmaxt\",output[:,1])\n",
        "   return output,temp,A\n",
        "                         #output = Y_predicted , temp = PreActivation  A = postActivation\n",
        "\n",
        "\n",
        "def find_Actuall_Y():\n",
        "  X,Y = x_trainFlat.shape\n",
        "  # print(X)\n",
        "  Y_actual=[]\n",
        "  for i in range(X):\n",
        "    tempX =x_trainFlat[i]\n",
        "    tempY = y_train[i]\n",
        "    vector = np.zeros(10)             # creating a vector of size 10 and assigning '1' to the correct index of trainFlat[i]\n",
        "    vector[tempY] =1\n",
        "    Y_actual.append(vector)\n",
        "    # #outputY=softmax(feedForward(x_trainFlat[i]))\n",
        "    # Y_predicted.append(outputY)\n",
        "    # loss = -np.mean(np.sum(Y_actual * np.log(Y_predicted),axis =1))          #crossEntropy loss Caluclation\n",
        "  return Y_actual\n",
        "\n",
        "\n",
        "def predict(W, b, x):\n",
        "    x = x.reshape(784, 1)\n",
        "    A = x\n",
        "    for i in range(len(W)-1):\n",
        "        Z = np.dot(W[i], A) + b[i]\n",
        "        A = np.maximum(Z, 0) # ReLU activation\n",
        "    Z = np.dot(W[-1], A) + b[-1]\n",
        "    y_pred = softmax(Z) # softmax activation\n",
        "    return y_pred\n",
        "\n",
        "layers = int(input(\"enter number of Hidden layers:\"))\n",
        "layers = int(layers)\n",
        "Y_actual = find_Actuall_Y()\n",
        "Y_actual = np.array(Y_actual)\n",
        "Y_actual = Y_actual.T\n",
        "numberOfneurons =[]\n",
        "for i in range(layers):\n",
        "  temp = int(input(\"Enter number of neurons:\"))\n",
        "  numberOfneurons.append(temp)\n",
        "\n",
        "W,b =Initalize_Wb(layers,numberOfneurons)\n",
        "\n",
        "activation = input(\"enter activation functions ReLU,sigmoid,tanh :\")\n",
        "Y_predicted, Intermediate_Activation, A = feedForward(x_trainFlat.T,W,b,layers,activation)\n",
        "\n",
        "Index = int(input(\"Enter Index of an Image {range: 0-9999}:\"))\n",
        "image = x_testFlat[Index]\n",
        "image = np.reshape(image, (784, 1))\n",
        "probablity = predict(W,b,image)\n",
        "print(probablity)"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvinashKushwah/cs6910_assignment1/blob/main/Question4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_oMtPmCeS3",
        "outputId": "7017f341-a911-48de-dfd3-b10ff73a1067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(y_train.shape)\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5eb7NvCAbX",
        "outputId": "28342f0f-904e-465a-eb95-e9c9eaf98e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Normalize the image pixel intesity within a range of 0-1\n",
        "x_trainFlat = x_train.reshape(len(x_train),784).astype('float32')/255\n",
        "print(x_trainFlat.shape)\n",
        "x_testFlat = x_test.reshape(len(x_test),784).astype('float32')/255\n",
        "print(x_testFlat.shape)\n",
        "#validate also \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XuBTUkAWG_Km"
      },
      "outputs": [],
      "source": [
        "# Activation Function\n",
        "\n",
        "def sigmoid(x):\n",
        "   \n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        " \n",
        "def sigmoid_diff(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def ReLU_deriv(x):\n",
        "    return x > 0\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_deriv(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "def softmax1(Z):\n",
        "  x = np.exp(Z) / (sum(np.exp(Z)) + 1e-9)\n",
        "  x = np.clip(x, -500, 500)\n",
        "  return x\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "80ZYFHeQIy05"
      },
      "outputs": [],
      "source": [
        "# layers = int(input(\"enter number of Hidden layers:\"))\n",
        "# layers = int(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5FC3bkmu2bs6"
      },
      "outputs": [],
      "source": [
        "# layers = input(\"enter number of Hidden layers:\")\n",
        "# layers = int(layers)\n",
        "# numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "# W = []                            #store the Weight matrix at each layer\n",
        "# b = []                            #store the bias vector at each layer\n",
        "\n",
        "\n",
        "# for i in range(layers):                       \n",
        "#   randomNumber = np.random.randint(4,10)\n",
        "#   numberOfNeurons.append(randomNumber)\n",
        "#   if( i== 0):\n",
        "#     w_temp =np.random.rand(randomNumber,784)\n",
        "#     b_temp = np.random.rand(randomNumber)\n",
        "#     W.append(w_temp)\n",
        "#     b.append(b_temp)\n",
        "#   else:\n",
        "#     w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])\n",
        "#     b_temp = np.random.rand(numberOfNeurons[i])\n",
        "#     W.append(w_temp);\n",
        "#     b.append(b_temp)\n",
        "\n",
        "# output_w = np.random.rand(10,numberOfNeurons[layers-1])\n",
        "# output_b = np.random.rand(10)\n",
        "# W.append(output_w)\n",
        "# b.append(output_b)\n",
        "\n",
        "# #print(len(W))\n",
        "# #print(len(b))\n",
        "# #print(W[0].shape)\n",
        "# # def randomIntialization(dimx,dimy):\n",
        "# #   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "# #   b_temp = np.random.randint(10,size=(dimx))\n",
        "# #   return W_temp, b_temp\n",
        "\n",
        "\n",
        "\n",
        "# def feedForward(input_data):\n",
        "    \n",
        "#    Y= sigmoid(np.dot(W[0],input_data) + b[0])\n",
        "#    for i in range(1,layers):\n",
        "#       Y = sigmoid(np.dot(W[i],Y)) + b[i]\n",
        "  \n",
        "#    output = sigmoid(np.dot(W[layers],Y)) + b[layers]\n",
        "#    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# output = feedForward(x_trainFlat[0])\n",
        "# outputSoftmax = softmax(output)\n",
        "# #print(output.shape)\n",
        "# print(outputSoftmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6pl3IeUQQ6cC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# numberOfNeurons =[] \n",
        "# for i in range(layers):\n",
        "#   temp = int(input(\"number of neurons:\"))\n",
        "#   numberOfNeurons.append(temp)\n",
        "def Initalize_Wb(layers,numberOfNeurons):\n",
        "  #layers = int(input(\"enter number of Hidden layers:\"))\n",
        "  #layers = int(layers)\n",
        " # numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "  W = []                            #store the Weight matrix at each layer\n",
        "  b = []                            #store the bias vector at each layer\n",
        "  for i in range(layers):                       \n",
        "    #randomNumber = np.random.randint(64,256)\n",
        "   # numberOfNeurons.append(randomNumber)\n",
        "    if( i== 0):\n",
        "      w_temp =np.random.rand(numberOfNeurons[i],784)-0.5\n",
        "      b_temp = np.random.rand(numberOfNeurons[i],1)-0.5\n",
        "      W.append(w_temp)\n",
        "      b.append(b_temp)\n",
        "    else:\n",
        "      w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])-0.5\n",
        "      b_temp = np.random.rand(numberOfNeurons[i],1)-0.5\n",
        "      W.append(w_temp);\n",
        "      b.append(b_temp)\n",
        "\n",
        "  print(numberOfNeurons)\n",
        "  output_w = np.random.rand(10,numberOfNeurons[layers-1])-0.5\n",
        "  output_b = np.random.rand(10,1)-0.5\n",
        "  W.append(output_w)\n",
        "  b.append(output_b)\n",
        "  return W,b\n",
        "\n",
        "def xavier_init(layers, numberOfNeurons):\n",
        "    W = []\n",
        "    b = []\n",
        "    for i in range(layers):\n",
        "        if i == 0:\n",
        "            # Input layer\n",
        "            w_temp = np.random.randn(numberOfNeurons[i], 784) / np.sqrt(784)\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        else:\n",
        "            # Hidden layers\n",
        "            w_temp = np.random.randn(numberOfNeurons[i], numberOfNeurons[i-1]) / np.sqrt(numberOfNeurons[i-1])\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        W.append(w_temp)\n",
        "        b.append(b_temp)\n",
        "    # Output layer\n",
        "    output_w = np.random.randn(10, numberOfNeurons[layers-1]) / np.sqrt(numberOfNeurons[layers-1])\n",
        "    output_b = np.zeros((10, 1))\n",
        "    W.append(output_w)\n",
        "    b.append(output_b)\n",
        "    return W, b\n",
        "\n",
        "\n",
        "def feedForward(input_data,W,b,layers,activation):\n",
        "  \n",
        "   output =[]\n",
        "   temp =[]\n",
        "   A =[]\n",
        "  #  print(\"hello\",b[0].shape)\n",
        "  #  print(b[0].reshape(-1,1).shape)\n",
        "   Y =np.dot(W[0],input_data) + b[0]\n",
        "   temp.append(Y)\n",
        "   if(activation == 'ReLU'):\n",
        "      Y=ReLU(Y)\n",
        "   elif(activation == 'sigmoid'):\n",
        "      Y=sigmoid(Y)\n",
        "   else:\n",
        "      Y = tanh(Y)\n",
        "        \n",
        "   A.append(Y)\n",
        "   for i in range(1,layers):\n",
        "      Y =np.dot(W[i],Y) + b[i]\n",
        "      temp.append(Y)\n",
        "      if(activation == 'ReLU'):\n",
        "        Y=ReLU(Y)\n",
        "        A.append(Y)\n",
        "      elif(activation == 'sigmoid'):\n",
        "        Y=sigmoid(Y)\n",
        "        A.append(Y)\n",
        "      else:\n",
        "        Y = tanh(Y)\n",
        "        A.append(Y)\n",
        "\n",
        "   temp2=np.dot(W[layers],Y) + b[layers]\n",
        "   temp.append(temp2)\n",
        "   if(activation == 'ReLU'):\n",
        "     output=(softmax1(ReLU(temp2)))\n",
        "   elif(activation == 'sigmoid'):\n",
        "     output=(softmax1(sigmoid(temp2)))\n",
        "   else:\n",
        "     output=(softmax1(tanh(temp2)))\n",
        "\n",
        "   #print(\"softmaxt\",output[:,1])\n",
        "   return output,temp,A\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmRsw01N1gS",
        "outputId": "f9ce9203-b7bd-477d-8978-f06ee298008c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 60000)\n"
          ]
        }
      ],
      "source": [
        "def find_Actuall_Y():\n",
        "  X,Y = x_trainFlat.shape\n",
        "  # print(X)\n",
        "  Y_actual=[]\n",
        "  for i in range(X):\n",
        "    tempX =x_trainFlat[i]\n",
        "    tempY = y_train[i]\n",
        "    vector = np.zeros(10)             # creating a vector of size 10 and assigning '1' to the correct index of trainFlat[i]\n",
        "    vector[tempY] =1\n",
        "    Y_actual.append(vector)\n",
        "    # #outputY=softmax(feedForward(x_trainFlat[i]))\n",
        "    # Y_predicted.append(outputY)\n",
        "    # loss = -np.mean(np.sum(Y_actual * np.log(Y_predicted),axis =1))          #crossEntropy loss Caluclation\n",
        "  return Y_actual\n",
        "\n",
        "\n",
        "Y_actual = find_Actuall_Y()\n",
        "Y_actual = np.array(Y_actual)\n",
        "Y_actual = Y_actual.T\n",
        "print(Y_actual.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Assuming Y_predicted and Y_actual are 10x60000 arrays\n",
        "def cross_entropy_loss(Y_actual, Y_predicted):\n",
        "    # Clip predicted values to avoid log(0) numerical instability\n",
        "    Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Compute cross-entropy loss\n",
        "    N = Y_actual.shape[1] # number of samples\n",
        "    loss = -1/N * np.sum(Y_actual * np.log(Y_predicted))\n",
        "    return loss\n",
        "\n",
        "# error = cross_entropy_loss(Y_actual, Y_predicted)\n",
        "# print(error)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "dcBVLnv5ftzg"
      },
      "outputs": [],
      "source": [
        "#Backpropogation Function\n",
        "\n",
        "def backpropagation(X, Y_actual, Y_predicted, Intermediate_Activation,A,W,b,activation):\n",
        "    dW = [None] * (len(W)) # gradient of weight matrix at each layer\n",
        "    db = [None] * (len(b)) # gradient of bias vector at each layer\n",
        "    \n",
        "    # Compute derivative of loss with respect to output\n",
        "    dL_dZ = Y_predicted - Y_actual\n",
        "    \n",
        "    # Compute gradients for output layer\n",
        "   #dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[-1].T)\n",
        "    dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, A[-1].T)\n",
        "    db[-1] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "   # print(dL_dZ.shape)\n",
        "    #print(db[-1].shape)\n",
        "    \n",
        "    # Backpropagate gradients through hidden layers\n",
        "    for l in reversed(range(1, len(W)-1)):\n",
        "        dL_dY = np.dot(W[l+1].T, dL_dZ)\n",
        "        # print(dL_dY.shape)\n",
        "        # print(Intermediate_Activation[l].shape)\n",
        "        # zy= sigmoid_diff(Intermediate_Activation[l-1])\n",
        "        # print(zy.shape)\n",
        "        dl_dz=[]\n",
        "        if(activation == 'ReLU'):\n",
        "          dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        elif(activation == 'sigmoid'):\n",
        "          dL_dZ = dL_dY * sigmoid_diff(Intermediate_Activation[l])\n",
        "        else:\n",
        "          dL_dZ = dL_dY * tanh_deriv(Intermediate_Activation[l])\n",
        "        # dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        dW[l] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[l-1].T)\n",
        "        db[l] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "\n",
        "    \n",
        "    # Compute gradients for input layer\n",
        "    dL_dY = np.dot(W[1].T, dL_dZ)\n",
        "    #dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[0])\n",
        "    if(activation == 'ReLU'):\n",
        "      dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[0])\n",
        "    elif(activation == 'sigmoid'):\n",
        "      dL_dZ = dL_dY * sigmoid_diff(Intermediate_Activation[0])\n",
        "    else:\n",
        "      dL_dZ = dL_dY * tanh_deriv(Intermediate_Activation[0])\n",
        "    dW[0] = 1/X.shape[0] * np.dot(dL_dZ, X)\n",
        "    db[0] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "    # print(\"insside update\",len(db))\n",
        "    # print(db[0].shape)\n",
        "    # print(db[1].shape)\n",
        "    # print(db[2].shape)\n",
        "    return dW, db\n",
        "\n",
        "# dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "# print(dW[0].shape,db[0].shape)\n",
        "# print(dW[1].shape,db[1].shape)\n",
        "# print(dW[2].shape,db[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Demu45MGTSNp"
      },
      "outputs": [],
      "source": [
        "def Update(W,b,dW,db,learningRate):\n",
        "\n",
        "  for i in range (len(W)):\n",
        "    W[i] = W[i] -learningRate*dW[i]\n",
        "   # print(\"before update\",b[i].shape,db[i].shape)\n",
        "    b[i] = b[i] -learningRate* db[i]\n",
        "    #print(\"after update\",b[i].shape, W[i].shape)\n",
        "  \n",
        "  return W,b\n",
        "\n",
        "\n",
        "#W,b=Update(W,b,dW,db,0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XaLQM-cQelX4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "OPeNQ0y-jijy"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(epochs,x_trainFlat,Y_actual,y_train,learning_rate,numberOfNeurons,num_layers,activation,W,b):\n",
        "    #W,b = Initalize_Wb()\n",
        "    print(len(W),len(b))\n",
        "    for i in range(epochs+1):\n",
        "        Y_predicted, Intermediate_Activation, A = feedForward(x_trainFlat.T,W,b,num_layers,activation)\n",
        "        dW, db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation,A,W,b,activation)\n",
        "        # print(\"x\",len(b),b[0].shape,b[1].shape,b[2].shape)\n",
        "        W,b = Update(W,b,dW,db,learning_rate)\n",
        "        #print(\"len of b\",len(b))\n",
        "        if i % 5 == 0:\n",
        "            print(\"iteration:\", i)\n",
        "           # y_pred = get_prediction(Y_predicted)\n",
        "            #print(\"Ypredicted\",Y_predicted[:,1])\n",
        "            #print(\"y_actual\",Y_actual[:,1])\n",
        "            #acuracy = getAccuracy(Y_predicted, Y_actual)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            print(\"Accuracy:\", get_accuracy(predictions,y_train)*100)\n",
        "    return W,b\n",
        "     \n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat, Y_actual,y_train,1000, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "6hQ5LsDPX5AP"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate, batch_size,W,b,numberOfNeurons,layers,activation):\n",
        "   # W, b = Initalize_Wb()\n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,layers,activation)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b,activation)\n",
        "            # Update the parameters using the batch gradient\n",
        "            W, b = Update(W, b, dW_batch, db_batch, learning_rate)\n",
        "        if i % 5 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,layers,activation)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    return W, b\n",
        "\n",
        "\n",
        "#W,b = stochastic_gradient_descent(x_trainFlat,Y_actual,y_train,100,0.1,64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "AdMYsitfTcIg"
      },
      "outputs": [],
      "source": [
        "def momentum_gradient_descent(x_trainFlat, Y_actual, y_train, epochs, learning_rate, batch_size, momentum,W,b,numberOfNeurons,num_layers,activation):\n",
        "    #W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # print(len(v_dW))\n",
        "    # print(len(v_db))\n",
        "    \n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,num_layers,activation)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b,activation)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [momentum * v_db + (1-momentum) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 5 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,num_layers,activation)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n",
        "#W, b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=1000, learning_rate=0.1,batch_size=64, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "EUpuizVXkVFN"
      },
      "outputs": [],
      "source": [
        "def nesterov_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,momentum,W,b,numberOfNeurons,num_layers,activation):\n",
        "    #W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # print(len(v_dW))\n",
        "    # print(len(v_db))\n",
        "    \n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,num_layers,activation)\n",
        "\n",
        "            # Compute the gradients at the lookahead position\n",
        "            W_lookahead = [w - momentum * v_dw for w, v_dw in zip(W, v_dW)]\n",
        "            b_lookahead = [b - momentum * v_db for b, v_db in zip(b, v_db)]\n",
        "            Y_predicted_lookahead, _, _ = feedForward(x_batch.T, W_lookahead, b_lookahead,num_layers,activation)\n",
        "            dW_batch_lookahead, db_batch_lookahead = backpropagation(x_batch, Y_actual_batch, Y_predicted_lookahead, Intermediate_Activation_batch, A_batch, W_lookahead, b_lookahead,activation)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch_lookahead)]\n",
        "            v_db = [momentum * v_db + (1-momentum) * db for v_db, db in zip(v_db, db_batch_lookahead)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 5 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,num_layers,activation)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JFdotIUVmkRJ"
      },
      "outputs": [],
      "source": [
        "def rmsprop(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, beta,epsilon):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize accumulated gradients to zero\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # set a small constant to avoid division by zero\n",
        "    #epsilon = 1e-8\n",
        "    \n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            \n",
        "            # Accumulate the squared gradients\n",
        "            s_dW = [beta * s_dw + (1-beta) * dw**2 for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta * s_db + (1-beta) * db**2 for s_db, db in zip(s_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the accumulated gradients\n",
        "            W = [w - learning_rate * dw / np.sqrt(s_dw + epsilon) for w, dw, s_dw in zip(W, dW_batch, s_dW)]\n",
        "            b = [b - learning_rate * db / np.sqrt(s_db + epsilon) for b, db, s_db in zip(b, db_batch, s_db)]\n",
        "            \n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, beta1, beta2, epsilon):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize velocities and squared gradients to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "\n",
        "            # Update velocities and squared gradients\n",
        "            v_dW = [beta1 * v_dw + (1 - beta1) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [beta1 * v_db + (1 - beta1) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            s_dW = [beta2 * s_dw + (1 - beta2) * np.square(dw) for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta2 * s_db + (1 - beta2) * np.square(db) for s_db, db in zip(s_db, db_batch)]\n",
        "\n",
        "            # Bias correction for velocities and squared gradients\n",
        "            v_dW_corrected = [v_dw / (1 - beta1**(i+1)) for i, v_dw in enumerate(v_dW)]\n",
        "            v_db_corrected = [v_db / (1 - beta1**(i+1)) for i, v_db in enumerate(v_db)]\n",
        "            s_dW_corrected = [s_dw / (1 - beta2**(i+1)) for i, s_dw in enumerate(s_dW)]\n",
        "            s_db_corrected = [s_db / (1 - beta2**(i+1)) for i, s_db in enumerate(s_db)]\n",
        "\n",
        "            # Update the parameters\n",
        "            W, b = Update(W, b, [(learning_rate * v_dW_corrected[i]) / (np.sqrt(s_dW_corrected[i]) + epsilon) for i in range(len(W))], [(learning_rate * v_db_corrected[i]) / (np.sqrt(s_db_corrected[i]) + epsilon) for i in range(len(b))],learning_rate)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ],
      "metadata": {
        "id": "sljSArfFxCgg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, beta1, beta2, epsilon):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize velocities and squared gradients to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # initialize the bias correction terms for velocities\n",
        "    m_dW = [np.zeros_like(w) for w in W]\n",
        "    m_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "\n",
        "            # Update velocities and squared gradients\n",
        "            v_dW = [beta1 * v_dw + (1 - beta1) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [beta1 * v_db + (1 - beta1) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            s_dW = [beta2 * s_dw + (1 - beta2) * np.square(dw) for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta2 * s_db + (1 - beta2) * np.square(db) for s_db, db in zip(s_db, db_batch)]\n",
        "\n",
        "            # Bias correction for velocities and squared gradients\n",
        "            v_dW_corrected = [v_dw / (1 - beta1**(i+1)) for i, v_dw in enumerate(v_dW)]\n",
        "            v_db_corrected = [v_db / (1 - beta1**(i+1)) for i, v_db in enumerate(v_db)]\n",
        "            s_dW_corrected = [s_dw / (1 - beta2**(i+1)) for i, s_dw in enumerate(s_dW)]\n",
        "            s_db_corrected = [s_db / (1 - beta2**(i+1)) for i, s_db in enumerate(s_db)]\n",
        "\n",
        "            # Compute the Nesterov momentum update\n",
        "            m_dW = [(beta1 * v_dW_corrected[i] + (1 - beta1) * dw) / (1 - beta1**(i+1)) for i, dw in enumerate(dW_batch)]\n",
        "            m_db = [(beta1 * v_db_corrected[i] + (1 - beta1) * db) / (1 - beta1**(i+1)) for i, db in enumerate(db_batch)]\n",
        "\n",
        "            # Compute the update for\n",
        "            m_dW = [(beta1 * v_dW_corrected[i] + (1 - beta1) * dw) / (1 - beta1**(i+1)) for i, dw in enumerate(dW_batch)]\n",
        "            m_db = [(beta1 * v_db_corrected[i] + (1 - beta1) * db) / (1 - beta1**(i+1)) for i, db in enumerate(db_batch)]\n",
        "            m_dW_corrected = [(beta1 * v_dW_corrected[i] + (1 - beta1) * dw) / (1 - beta1**(i+1)) for i, dw in enumerate(m_dW)]\n",
        "            m_db_corrected = [(beta1 * v_db_corrected[i] + (1 - beta1) * db) / (1 - beta1**(i+1)) for i, db in enumerate(m_db)]\n",
        "            v_dW_corrected = [(beta2 * v_dW_corrected[i] + (1 - beta2) * np.square(dw)) for i, dw in enumerate(dW_batch)]\n",
        "            v_db_corrected = [(beta2 * v_db_corrected[i] + (1 - beta2) * np.square(db)) for i, db in enumerate(db_batch)]\n",
        "            v_dW_corrected = [v_dW_corrected[i] / (1 - beta2**(i+1)) for i in range(len(v_dW_corrected))]\n",
        "            v_db_corrected = [v_db_corrected[i] / (1 - beta2**(i+1)) for i in range(len(v_db_corrected))]\n",
        "            W, b = Update(W, b, [((learning_rate * m_dW_corrected[i]) / (np.sqrt(v_dW_corrected[i]) + epsilon)) if np.greater_equal(v_dW_corrected[i], 0) else ((learning_rate * m_dW_corrected[i]) / (np.sqrt(abs(v_dW_corrected[i])) + epsilon)) for i in range(len(W))], [((learning_rate * m_db_corrected[i]) / (np.sqrt(v_db_corrected[i]) + epsilon)) if v_db_corrected[i] >= 0 else ((learning_rate * m_db_corrected[i]) / (np.sqrt(abs(v_db_corrected[i])) + epsilon)) for i in range(len(b))],learning_rate)\n",
        "\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    return W,b\n"
      ],
      "metadata": {
        "id": "pNYfu_NW1QMz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9oopKx7uTi-6"
      },
      "outputs": [],
      "source": [
        "def prediction_Test(input_data,W,b,num_layers,activation):\n",
        "  Y_predicted, _, _ = feedForward(input_data.T, W, b,num_layers,activation)\n",
        "  predictions = get_predictions(Y_predicted)\n",
        "  accuracy = get_accuracy(predictions, y_test) * 100\n",
        "  print(f\"Test_Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"GD\")\n",
        "# W,b =gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.0061)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"sGD\")\n",
        "# W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10,learning_rate=0.1, batch_size=64)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"MGD\")\n",
        "# W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, momentum=0.9)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"NGD\")\n",
        "# W,b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, momentum=0.9, batch_size=64)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"rms\")\n",
        "# W,b = rmsprop(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, beta=0.9, epsilon=1e-2)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"adam\")\n",
        "# W,b = adam(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"nadam\")\n",
        "# W,b= nadam(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.01, batch_size=32, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "# prediction_Test(x_testFlat,W,b)\n"
      ],
      "metadata": {
        "id": "EFo3Ug_dBWY9"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neuralNetwork(epochs,batch_size,optimizer,learning_rate,beta,beta1,beta2,epsilon,weight_init,num_layers,hidden_size,activation,momentum):\n",
        "  numberOfNeurons =[] \n",
        "  for i in range(num_layers):\n",
        "      temp = hidden_size\n",
        "      numberOfNeurons.append(temp)\n",
        "  W=[]\n",
        "  b=[]\n",
        "  if weight_init == 'random':\n",
        "    W,b = Initalize_Wb(num_layers,numberOfNeurons)\n",
        "  else:\n",
        "    W,b = xavier_init(num_layers, numberOfNeurons)\n",
        "\n",
        "  if optimizer == 'GD':\n",
        "      W,b =gradient_descent(epochs,x_trainFlat,Y_actual, y_train,learning_rate,numberOfNeurons,num_layers,activation,W,b)\n",
        "      prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer =='sgd':\n",
        "    W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate, batch_size,W,b,numberOfNeurons,num_layers,activation)\n",
        "    prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'momentum':\n",
        "    W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,momentum,W,b,numberOfNeurons,num_layers,activation)\n",
        "    prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'nestrov':\n",
        "    W,b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,momentum,W,b,numberOfNeurons,num_layers,activation)\n",
        "    prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'rmsprop':\n",
        "     W,b = rmsprop(x_trainFlat, Y_actual, y_train,epochs, learning_rate, batch_size,beta, epsilon,W,b,numberOfNeurons,num_layers,activation)\n",
        "     prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'adam':\n",
        "     W,b = adam(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,beta1,beta2,epsilon,W,b,numberOfNeurons,num_layers,activation)\n",
        "     prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "neuralNetwork(epochs=10,batch_size= 64, optimizer ='momentum', learning_rate = 0.01,beta =0.1,beta1 = 0.5,beta2 =0.5,epsilon =1e-8,weight_init='xavier',num_layers=5,hidden_size=64,activation='tanh',momentum=0.9)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jeeeavu4pvhz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "c9fb6dba-6fdd-4c52-b12c-1cdf7d2a02ae"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 0 1 ... 3 0 7] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 65.05%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 5: accuracy = 64.31%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-020c73ac37a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mneuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xavier'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-020c73ac37a1>\u001b[0m in \u001b[0;36mneuralNetwork\u001b[0;34m(epochs, batch_size, optimizer, learning_rate, beta, beta1, beta2, epsilon, weight_init, num_layers, hidden_size, activation, momentum)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprediction_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_testFlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'momentum'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainFlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumberOfNeurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprediction_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_testFlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nestrov'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-caed033b4f76>\u001b[0m in \u001b[0;36mmomentum_gradient_descent\u001b[0;34m(x_trainFlat, Y_actual, y_train, epochs, learning_rate, batch_size, momentum, W, b, numberOfNeurons, num_layers, activation)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Compute the forward pass and backpropagation for the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mY_predicted_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntermediate_Activation_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdW_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_actual_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predicted_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntermediate_Activation_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Update velocities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-607ccd0bc14e>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(X, Y_actual, Y_predicted, Intermediate_Activation, A, W, b, activation)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mdL_dZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_dY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtanh_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntermediate_Activation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# print(\"insside update\",len(db))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "CysxuvIRv6za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config={\n",
        "    'method' : 'bayes' ,\n",
        "    'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\n",
        "    'parameters' : {\n",
        "        'epochs' : { 'values' : [5,10,20] },\n",
        "        'n_hidden_layers' : {'values' : [3,4,5]},\n",
        "        'n_hidden_layer_size' : { 'values' : [16,32,64,128,256]},\n",
        "        'batch_size' : { 'values' : [16,32,64,128]},\n",
        "        'learning_rate' : { 'values' : [0.001, 0.0001,0.0002,0.0003]},\n",
        "        'optimizer' : { 'values' : [\"sgd\", \"mgd\"] },\n",
        "        'activations' : { 'values' : [\"sigmoid\", \"tanh\", \"relu\"] },\n",
        "        #'loss_function' : {'values' : ['cross_entropy']},\n",
        "        'weight_ini' : {'values' : ['random']},\n",
        "       # 'weight_decay' : { 'values' : [0,0.0005]},\n",
        "        'beta': {'values' : [0.5]},\n",
        "        'beta1': {'values' : [0.5]},\n",
        "        'beta2': {'values' : [0.5]},\n",
        "        'momentum': {'values' : [0.5]},\n",
        "        'epsilon' : {'values' : [1e-2]}\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "def tr():\n",
        "  config_default={\n",
        "      'weight_ini':'bayes',\n",
        "      'n_hidden_layers':3,\n",
        "      'n_hidden_layer_size':32,\n",
        "      'optimizer':'sgd',\n",
        "      'learning_rate':0.01,\n",
        "      'epoch':10,\n",
        "      'batch_size':32\n",
        "  }\n",
        "  wandb.init(config=config_default)\n",
        "\n",
        "  c= wandb.config\n",
        "  name = \"op_\"+str(c.optimizer)+\"ac\"+str(c.activations)+\"hl\"+str(c.n_hidden_layers)+\"hls\"+str(c.n_hidden_layer_size)+\"ep\"+str(c.epochs)+\"n\"+str(c.learning_rate)+\"bs\"+str(c.batch_size)+\"wi\"+str(c.weight_ini)\n",
        "  wandb.init(name=name)\n",
        "\n",
        "  hn = c.n_hidden_layer_size\n",
        "  hl = c.n_hidden_layers\n",
        "  act = c.activations\n",
        "  opt = c.optimizer\n",
        "  ep = c.epochs\n",
        "  bs = c.batch_size\n",
        "  lr = c.learning_rate\n",
        "  wi = c.weight_ini\n",
        "  mm =c.momentum\n",
        "  b =c.beta\n",
        "  b1=c.beta1\n",
        "  b2=c.beta2\n",
        "  e =c.epsilon\n",
        "  neuralNetwork(ep,bs,opt, lr,b,b1,b2,e,wi,hl,hn,act,mm)\n",
        "  return\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"dl_assignment\")\n",
        "wandb.agent(sweep_id, function=tr,count=3)"
      ],
      "metadata": {
        "id": "Nir4rvaNqIkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5dk0_OMzqM6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvinashKushwah/cs6910_assignment1/blob/main/Question4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_oMtPmCeS3",
        "outputId": "c2dde08c-3a28-4a17-fa29-6e73af13bc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# !wandb login\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(y_train.shape)\n",
        "\n",
        "x_train, x_validate, y_train, y_validate = train_test_split( x_train, y_train, test_size=0.1, random_state=42)\n",
        "# Define the class names\n",
        "\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5eb7NvCAbX",
        "outputId": "85bdc13d-d568-46e2-ce86-0d0363758862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 784)\n",
            "(54000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Normalize the image pixel intesity within a range of 0-1\n",
        "x_trainFlat = x_train.reshape(len(x_train),784).astype('float32')/255\n",
        "print(x_trainFlat.shape)\n",
        "\n",
        "x_validateFlat = x_validate.reshape(len(x_validate),784).astype('float32')/255\n",
        "print(x_trainFlat.shape)\n",
        "\n",
        "x_testFlat = x_test.reshape(len(x_test),784).astype('float32')/255\n",
        "print(x_testFlat.shape)\n",
        "#validate also \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "XuBTUkAWG_Km"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def ReLU(z):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def ReLU_deriv(z):\n",
        "    \"\"\"Derivative of ReLU activation function\"\"\"\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_diff(z):\n",
        "    \"\"\"Derivative of sigmoid activation function\"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh(z):\n",
        "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_deriv(z):\n",
        "    \"\"\"Derivative of hyperbolic tangent activation function\"\"\"\n",
        "    return 1 - np.square(np.tanh(z))\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"Softmax activation function to handle multiclass classification\"\"\"\n",
        "   # print(z[0].shape)\n",
        "    shift_z = z - np.max(z, axis=0, keepdims=True)  # avoid overflow\n",
        "    exp_z = np.exp(shift_z)\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)  # avoid underflow\n",
        "# def softmax(z):\n",
        "#   z=z.T\n",
        "#   k=[]\n",
        "#   for i in range(z.shape[0]):\n",
        "#     sum=0\n",
        "#     z[i]=z[i]-z[i][np.argmax(z[i])]\n",
        "#     for j in range(z.shape[1]):\n",
        "#         sum+=np.exp(z[i][j])\n",
        "#     k.append(np.exp(z[i])/sum)\n",
        "#   k=np.array(k)\n",
        "#   return k.T\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "80ZYFHeQIy05"
      },
      "outputs": [],
      "source": [
        "# layers = int(input(\"enter number of Hidden layers:\"))\n",
        "# layers = int(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "5FC3bkmu2bs6"
      },
      "outputs": [],
      "source": [
        "# layers = input(\"enter number of Hidden layers:\")\n",
        "# layers = int(layers)\n",
        "# numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "# W = []                            #store the Weight matrix at each layer\n",
        "# b = []                            #store the bias vector at each layer\n",
        "\n",
        "\n",
        "# for i in range(layers):                       \n",
        "#   randomNumber = np.random.randint(4,10)\n",
        "#   numberOfNeurons.append(randomNumber)\n",
        "#   if( i== 0):\n",
        "#     w_temp =np.random.rand(randomNumber,784)\n",
        "#     b_temp = np.random.rand(randomNumber)\n",
        "#     W.append(w_temp)\n",
        "#     b.append(b_temp)\n",
        "#   else:\n",
        "#     w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])\n",
        "#     b_temp = np.random.rand(numberOfNeurons[i])\n",
        "#     W.append(w_temp);\n",
        "#     b.append(b_temp)\n",
        "\n",
        "# output_w = np.random.rand(10,numberOfNeurons[layers-1])\n",
        "# output_b = np.random.rand(10)\n",
        "# W.append(output_w)\n",
        "# b.append(output_b)\n",
        "\n",
        "# #print(len(W))\n",
        "# #print(len(b))\n",
        "# #print(W[0].shape)\n",
        "# # def randomIntialization(dimx,dimy):\n",
        "# #   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "# #   b_temp = np.random.randint(10,size=(dimx))\n",
        "# #   return W_temp, b_temp\n",
        "\n",
        "\n",
        "\n",
        "# def feedForward(input_data):\n",
        "    \n",
        "#    Y= sigmoid(np.dot(W[0],input_data) + b[0])\n",
        "#    for i in range(1,layers):\n",
        "#       Y = sigmoid(np.dot(W[i],Y)) + b[i]\n",
        "  \n",
        "#    output = sigmoid(np.dot(W[layers],Y)) + b[layers]\n",
        "#    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# output = feedForward(x_trainFlat[0])\n",
        "# outputSoftmax = softmax(output)\n",
        "# #print(output.shape)\n",
        "# print(outputSoftmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "6pl3IeUQQ6cC"
      },
      "outputs": [],
      "source": [
        "def Initalize_Wb(layers, numberOfNeurons):\n",
        "    W = []\n",
        "    b = []\n",
        "    for i in range(layers):\n",
        "        if i == 0:\n",
        "            # Input layer\n",
        "            w_temp = np.random.randn(numberOfNeurons[i], 784) * np.sqrt(2/784)\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        else:\n",
        "            # Hidden layers\n",
        "            w_temp = np.random.randn(numberOfNeurons[i], numberOfNeurons[i-1])*np.sqrt(2/numberOfNeurons[i-1])\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        W.append(w_temp)\n",
        "        b.append(b_temp)\n",
        "    # Output layer\n",
        "    output_w = np.random.randn(10, numberOfNeurons[-1]) * np.sqrt(2/numberOfNeurons[-1])\n",
        "    output_b = np.zeros((10, 1))\n",
        "    W.append(output_w)\n",
        "    b.append(output_b)\n",
        "    return W, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def xavier_init(layers, numberOfNeurons):\n",
        "    W = []\n",
        "    b = []\n",
        "    for i in range(layers):\n",
        "        if i == 0:\n",
        "            # Input layer\n",
        "            w_temp = np.random.randn(numberOfNeurons[i], 784) * np.sqrt(1/784)\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        else:\n",
        "            # Hidden layers\n",
        "            w_temp = np.random.randn(numberOfNeurons[i], numberOfNeurons[i-1]) * np.sqrt(1/numberOfNeurons[i-1])\n",
        "            b_temp = np.zeros((numberOfNeurons[i], 1))\n",
        "        W.append(w_temp)\n",
        "        b.append(b_temp)\n",
        "    # Output layer\n",
        "    output_w = np.random.randn(10, numberOfNeurons[-1]) * np.sqrt(1/numberOfNeurons[-1])\n",
        "    output_b = np.zeros((10, 1))\n",
        "    W.append(output_w)\n",
        "    b.append(output_b)\n",
        "    return W, b\n",
        "\n",
        "\n",
        "def feedForward(input_data,W,b,layers,activation):\n",
        "  \n",
        "   output =[]\n",
        "   temp =[]\n",
        "   A =[]\n",
        "  #  print(\"hello\",b[0].shape)\n",
        "  #  print(b[0].reshape(-1,1).shape)\n",
        "   Y =np.dot(W[0],input_data) + b[0]\n",
        "   temp.append(Y)\n",
        "   if(activation == 'ReLU'):\n",
        "      Y=ReLU(Y)\n",
        "   elif(activation == 'sigmoid'):\n",
        "      Y=sigmoid(Y)\n",
        "   else:\n",
        "      Y = tanh(Y)\n",
        "        \n",
        "   A.append(Y)\n",
        "   for i in range(1,layers):\n",
        "      Y =np.dot(W[i],Y) + b[i]\n",
        "      temp.append(Y)\n",
        "      if(activation == 'ReLU'):\n",
        "        Y=ReLU(Y)\n",
        "        A.append(Y)\n",
        "      elif(activation == 'sigmoid'):\n",
        "        Y=sigmoid(Y)\n",
        "        A.append(Y)\n",
        "      else:\n",
        "        Y = tanh(Y)\n",
        "        A.append(Y)\n",
        "\n",
        "   temp2=np.dot(W[layers],Y) + b[layers]\n",
        "   temp.append(temp2)\n",
        "  #  if(activation == 'ReLU'):\n",
        "  #    output=(softmax(temp2))\n",
        "  #  elif(activation == 'sigmoid'):\n",
        "  #    output=(softmax(sigmoid(temp2)))\n",
        "  #  else:\n",
        "  #    output=(softmax(tanh(temp2)))\n",
        "   output=softmax(temp2)\n",
        "   #print(\"softmaxt\",output[:,1])\n",
        "   return output,temp,A\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmRsw01N1gS",
        "outputId": "5cee3a01-718c-4645-ea24-362a0349ea73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 54000)\n"
          ]
        }
      ],
      "source": [
        "def find_Actuall_Y():\n",
        "  X,Y = x_trainFlat.shape\n",
        "  # print(X)\n",
        "  Y_actual=[]\n",
        "  for i in range(X):\n",
        "    tempX =x_trainFlat[i]\n",
        "    tempY = y_train[i]\n",
        "    vector = np.zeros(10)             # creating a vector of size 10 and assigning '1' to the correct index of trainFlat[i]\n",
        "    vector[tempY] =1\n",
        "    Y_actual.append(vector)\n",
        "    # #outputY=softmax(feedForward(x_trainFlat[i]))\n",
        "    # Y_predicted.append(outputY)\n",
        "    # loss = -np.mean(np.sum(Y_actual * np.log(Y_predicted),axis =1))          #crossEntropy loss Caluclation\n",
        "  return Y_actual\n",
        "\n",
        "\n",
        "Y_actual = find_Actuall_Y()\n",
        "Y_actual = np.array(Y_actual)\n",
        "Y_actual = Y_actual.T\n",
        "print(Y_actual.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Assuming Y_predicted and Y_actual are 10x60000 arrays\n",
        "def cross_entropy_loss(Y_actual, Y_predicted):\n",
        "    # Clip predicted values to avoid log(0) numerical instability\n",
        "    Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Compute cross-entropy loss\n",
        "    N = Y_actual.shape[1] # number of samples\n",
        "    loss = -1/N * np.sum(Y_actual * np.log(Y_predicted))\n",
        "    return loss\n",
        "\n",
        "# error = cross_entropy_loss(Y_actual, Y_predicted)\n",
        "# print(error)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "dcBVLnv5ftzg"
      },
      "outputs": [],
      "source": [
        "#Backpropogation Function\n",
        "\n",
        "def backpropagation(X, Y_actual, Y_predicted, Intermediate_Activation,A,W,b,activation):\n",
        "    dW = [None] * (len(W)) # gradient of weight matrix at each layer\n",
        "    db = [None] * (len(b)) # gradient of bias vector at each layer\n",
        "    \n",
        "    # Compute derivative of loss with respect to output\n",
        "    dL_dZ = Y_predicted - Y_actual\n",
        "    \n",
        "    # Compute gradients for output layer\n",
        "   #dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[-1].T)\n",
        "    dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, A[-1].T)\n",
        "    db[-1] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "   # print(dL_dZ.shape)\n",
        "    #print(db[-1].shape)\n",
        "    \n",
        "    # Backpropagate gradients through hidden layers\n",
        "    for l in reversed(range(1, len(W)-1)):\n",
        "        dL_dY = np.dot(W[l+1].T, dL_dZ)\n",
        "        # print(dL_dY.shape)\n",
        "        # print(Intermediate_Activation[l].shape)\n",
        "        # zy= sigmoid_diff(Intermediate_Activation[l-1])\n",
        "        # print(zy.shape)\n",
        "        dl_dz=[]\n",
        "        if(activation == 'ReLU'):\n",
        "          dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        elif(activation == 'sigmoid'):\n",
        "          dL_dZ = dL_dY * sigmoid_diff(Intermediate_Activation[l])\n",
        "        else:\n",
        "          dL_dZ = dL_dY * tanh_deriv(Intermediate_Activation[l])\n",
        "        # dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        dW[l] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[l-1].T)\n",
        "        db[l] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "\n",
        "    \n",
        "    # Compute gradients for input layer\n",
        "    dL_dY = np.dot(W[1].T, dL_dZ)\n",
        "    #dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[0])\n",
        "    if(activation == 'ReLU'):\n",
        "      dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[0])\n",
        "    elif(activation == 'sigmoid'):\n",
        "      dL_dZ = dL_dY * sigmoid_diff(Intermediate_Activation[0])\n",
        "    else:\n",
        "      dL_dZ = dL_dY * tanh_deriv(Intermediate_Activation[0])\n",
        "    dW[0] = 1/X.shape[0] * np.dot(dL_dZ, X)\n",
        "    db[0] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "    # print(\"insside update\",len(db))\n",
        "    # print(db[0].shape)\n",
        "    # print(db[1].shape)\n",
        "    # print(db[2].shape)\n",
        "    return dW, db\n",
        "\n",
        "# dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "# print(dW[0].shape,db[0].shape)\n",
        "# print(dW[1].shape,db[1].shape)\n",
        "# print(dW[2].shape,db[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "Demu45MGTSNp"
      },
      "outputs": [],
      "source": [
        "def Update(W,b,dW,db,learningRate):\n",
        "\n",
        "  for i in range (len(W)):\n",
        "    W[i] = W[i] -learningRate*dW[i]\n",
        "   # print(\"before update\",b[i].shape,db[i].shape)\n",
        "    b[i] = b[i] -learningRate* db[i]\n",
        "    #print(\"after update\",b[i].shape, W[i].shape)\n",
        "  \n",
        "  return W,b\n",
        "\n",
        "\n",
        "#W,b=Update(W,b,dW,db,0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "XaLQM-cQelX4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "OPeNQ0y-jijy"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(epochs,x_trainFlat,Y_actual,y_train,learning_rate,numberOfNeurons,num_layers,activation,W,b):\n",
        "    #W,b = Initalize_Wb()\n",
        "    print(len(W),len(b))\n",
        "    for i in range(epochs+1):\n",
        "        Y_predicted, Intermediate_Activation, A = feedForward(x_trainFlat.T,W,b,num_layers,activation)\n",
        "        dW, db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation,A,W,b,activation)\n",
        "        # print(\"x\",len(b),b[0].shape,b[1].shape,b[2].shape)\n",
        "        W,b = Update(W,b,dW,db,learning_rate)\n",
        "        #print(\"len of b\",len(b))\n",
        "        if i % 5 == 0:\n",
        "            print(\"iteration:\", i)\n",
        "           # y_pred = get_prediction(Y_predicted)\n",
        "            #print(\"Ypredicted\",Y_predicted[:,1])\n",
        "            #print(\"y_actual\",Y_actual[:,1])\n",
        "            #acuracy = getAccuracy(Y_predicted, Y_actual)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            print(\"Accuracy:\", get_accuracy(predictions,y_train)*100)\n",
        "    return W,b\n",
        "     \n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat, Y_actual,y_train,1000, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "6hQ5LsDPX5AP"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate, batch_size,W,b,numberOfNeurons,layers,activation):\n",
        "   # W, b = Initalize_Wb()\n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,layers,activation)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b,activation)\n",
        "            # Update the parameters using the batch gradient\n",
        "            W, b = Update(W, b, dW_batch, db_batch, learning_rate)\n",
        "      \n",
        "            # Compute the accuracy on the entire training set\n",
        "        Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,layers,activation)\n",
        "        predictions = get_predictions(Y_predicted)\n",
        "        accuracy = get_accuracy(predictions, y_train) *100\n",
        "        print(f\"Iteration {i}: accuracy = {accuracy:.2f}\")\n",
        "        Y_predicted_validate, _, _ = feedForward(x_validateFlat.T, W, b,layers,activation)\n",
        "        predictions_validate = get_predictions(Y_predicted_validate)\n",
        "        accuracy_val = get_accuracy(predictions_validate, y_validate) *100\n",
        "        print(f\"Iteration {i}: accuracy = {accuracy_val:.2f}\")\n",
        "       # wandb.log({\"train_accuracy\":accuracy} )\n",
        "    return W, b\n",
        "\n",
        "\n",
        "#W,b = stochastic_gradient_descent(x_trainFlat,Y_actual,y_train,100,0.1,64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPdK5hm07Q41"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "AdMYsitfTcIg"
      },
      "outputs": [],
      "source": [
        "def momentum_gradient_descent(x_trainFlat, Y_actual, y_train, epochs, learning_rate, batch_size, momentum,W,b,numberOfNeurons,num_layers,activation):\n",
        "    #W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # print(len(v_dW))\n",
        "    # print(len(v_db))\n",
        "    \n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,num_layers,activation)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b,activation)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [momentum * v_db + (1-momentum) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 5 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,num_layers,activation)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n",
        "#W, b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=1000, learning_rate=0.1,batch_size=64, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "EUpuizVXkVFN"
      },
      "outputs": [],
      "source": [
        "def nesterov_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,momentum,W,b,numberOfNeurons,num_layers,activation):\n",
        "    #W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # print(len(v_dW))\n",
        "    # print(len(v_db))\n",
        "    \n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            #Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,num_layers,activation)\n",
        "\n",
        "            # Compute the gradients at the lookahead position\n",
        "            #for all layers using loop\n",
        "            W_lookahead = [w - momentum * v_dw for w, v_dw in zip(W, v_dW)]\n",
        "            b_lookahead = [b - momentum * j for b, j in zip(b, v_db)]\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch= feedForward(x_batch.T, W_lookahead, b_lookahead,num_layers,activation)\n",
        "            dW_batch_lookahead, db_batch_lookahead = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W_lookahead, b_lookahead,activation)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch_lookahead)]\n",
        "            v_db = [momentum * j + (1-momentum) * db for j, db in zip(v_db, db_batch_lookahead)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 5 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,num_layers,activation)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "JFdotIUVmkRJ"
      },
      "outputs": [],
      "source": [
        "def rmsprop(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, beta,epsilon):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize accumulated gradients to zero\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # set a small constant to avoid division by zero\n",
        "    #epsilon = 1e-8\n",
        "    \n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            \n",
        "            # Accumulate the squared gradients\n",
        "            s_dW = [beta * s_dw + (1-beta) * dw**2 for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta * s_db + (1-beta) * db**2 for s_db, db in zip(s_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the accumulated gradients\n",
        "            W = [w - learning_rate * dw / np.sqrt(s_dw + epsilon) for w, dw, s_dw in zip(W, dW_batch, s_dW)]\n",
        "            b = [b - learning_rate * db / np.sqrt(s_db + epsilon) for b, db, s_db in zip(b, db_batch, s_db)]\n",
        "            \n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(x_trainFlat, Y_actual, y_train,epochs, learning_rate, batch_size, beta1, beta2, epsilon,W,b,numberOfNeurons,num_layers,activation):\n",
        "    #W, b = Initalize_Wb()\n",
        "    # initialize velocities and squared gradients to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    for i in range(epochs+1):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b,num_layers,activation)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b,activation)\n",
        "\n",
        "            # Update velocities and squared gradients\n",
        "            v_dW = [beta1 * v_dw + (1 - beta1) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [beta1 * v_db + (1 - beta1) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            s_dW = [beta2 * s_dw + (1 - beta2) * np.square(dw) for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta2 * s_db + (1 - beta2) * np.square(db) for s_db, db in zip(s_db, db_batch)]\n",
        "\n",
        "            # Bias correction for velocities and squared gradients\n",
        "            v_dW_corrected = [v_dw / (1 - beta1**(i+1)) for i, v_dw in enumerate(v_dW)]\n",
        "            v_db_corrected = [v_db / (1 - beta1**(i+1)) for i, v_db in enumerate(v_db)]\n",
        "            s_dW_corrected = [s_dw / (1 - beta2**(i+1)) for i, s_dw in enumerate(s_dW)]\n",
        "            s_db_corrected = [s_db / (1 - beta2**(i+1)) for i, s_db in enumerate(s_db)]\n",
        "\n",
        "            # Update the parameters\n",
        "            W, b = Update(W, b, [(learning_rate * v_dW_corrected[i]) / (np.sqrt(s_dW_corrected[i]) + epsilon) for i in range(len(W))], [(learning_rate * v_db_corrected[i]) / (np.sqrt(s_db_corrected[i]) + epsilon) for i in range(len(b))],learning_rate)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b,num_layers,activation)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ],
      "metadata": {
        "id": "sljSArfFxCgg"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, beta1, beta2, epsilon):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize velocities and squared gradients to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # initialize the bias correction terms for velocities\n",
        "    m_dW = [np.zeros_like(w) for w in W]\n",
        "    m_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "\n",
        "            # Update velocities and squared gradients\n",
        "            v_dW = [beta1 * v_dw + (1 - beta1) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [beta1 * v_db + (1 - beta1) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            s_dW = [beta2 * s_dw + (1 - beta2) * np.square(dw) for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta2 * s_db + (1 - beta2) * np.square(db) for s_db, db in zip(s_db, db_batch)]\n",
        "\n",
        "            # Bias correction for velocities and squared gradients\n",
        "            v_dW_corrected = [v_dw / (1 - beta1**(i+1)) for i, v_dw in enumerate(v_dW)]\n",
        "            v_db_corrected = [v_db / (1 - beta1**(i+1)) for i, v_db in enumerate(v_db)]\n",
        "            s_dW_corrected = [s_dw / (1 - beta2**(i+1)) for i, s_dw in enumerate(s_dW)]\n",
        "            s_db_corrected = [s_db / (1 - beta2**(i+1)) for i, s_db in enumerate(s_db)]\n",
        "\n",
        "            # Compute the Nesterov momentum update\n",
        "            m_dW = [(beta1 * v_dW_corrected[i] + (1 - beta1) * dw) / (1 - beta1**(i+1)) for i, dw in enumerate(dW_batch)]\n",
        "            m_db = [(beta1 * v_db_corrected[i] + (1 - beta1) * db) / (1 - beta1**(i+1)) for i, db in enumerate(db_batch)]\n",
        "\n",
        "            # Compute the update for\n",
        "            m_dW = [(beta1 * v_dW_corrected[i] + (1 - beta1) * dw) / (1 - beta1**(i+1)) for i, dw in enumerate(dW_batch)]\n",
        "            m_db = [(beta1 * v_db_corrected[i] + (1 - beta1) * db) / (1 - beta1**(i+1)) for i, db in enumerate(db_batch)]\n",
        "            m_dW_corrected = [(beta1 * v_dW_corrected[i] + (1 - beta1) * dw) / (1 - beta1**(i+1)) for i, dw in enumerate(m_dW)]\n",
        "            m_db_corrected = [(beta1 * v_db_corrected[i] + (1 - beta1) * db) / (1 - beta1**(i+1)) for i, db in enumerate(m_db)]\n",
        "            v_dW_corrected = [(beta2 * v_dW_corrected[i] + (1 - beta2) * np.square(dw)) for i, dw in enumerate(dW_batch)]\n",
        "            v_db_corrected = [(beta2 * v_db_corrected[i] + (1 - beta2) * np.square(db)) for i, db in enumerate(db_batch)]\n",
        "            v_dW_corrected = [v_dW_corrected[i] / (1 - beta2**(i+1)) for i in range(len(v_dW_corrected))]\n",
        "            v_db_corrected = [v_db_corrected[i] / (1 - beta2**(i+1)) for i in range(len(v_db_corrected))]\n",
        "            W, b = Update(W, b, [((learning_rate * m_dW_corrected[i]) / (np.sqrt(v_dW_corrected[i]) + epsilon)) if np.greater_equal(v_dW_corrected[i], 0) else ((learning_rate * m_dW_corrected[i]) / (np.sqrt(abs(v_dW_corrected[i])) + epsilon)) for i in range(len(W))], [((learning_rate * m_db_corrected[i]) / (np.sqrt(v_db_corrected[i]) + epsilon)) if v_db_corrected[i] >= 0 else ((learning_rate * m_db_corrected[i]) / (np.sqrt(abs(v_db_corrected[i])) + epsilon)) for i in range(len(b))],learning_rate)\n",
        "\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    return W,b\n"
      ],
      "metadata": {
        "id": "pNYfu_NW1QMz"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "9oopKx7uTi-6"
      },
      "outputs": [],
      "source": [
        "def prediction_Test(input_data,W,b,num_layers,activation):\n",
        "  Y_predicted, _, _ = feedForward(input_data.T, W, b,num_layers,activation)\n",
        "  predictions = get_predictions(Y_predicted)\n",
        "  accuracy = get_accuracy(predictions, y_test) * 100\n",
        "  print(f\"Test_Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"GD\")\n",
        "# W,b =gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.0061)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"sGD\")\n",
        "# W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10,learning_rate=0.1, batch_size=64)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"MGD\")\n",
        "# W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, momentum=0.9)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"NGD\")\n",
        "# W,b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, momentum=0.9, batch_size=64)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"rms\")\n",
        "# W,b = rmsprop(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, beta=0.9, epsilon=1e-2)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"adam\")\n",
        "# W,b = adam(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "# prediction_Test(x_testFlat,W,b)\n",
        "# print(\"nadam\")\n",
        "# W,b= nadam(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.01, batch_size=32, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "# prediction_Test(x_testFlat,W,b)\n"
      ],
      "metadata": {
        "id": "EFo3Ug_dBWY9"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neuralNetwork(epochs,batch_size,optimizer,learning_rate,beta,beta1,beta2,epsilon,weight_init,num_layers,hidden_size,activation,momentum):\n",
        "  numberOfNeurons =[] \n",
        "  for i in range(num_layers):\n",
        "      temp = hidden_size\n",
        "      numberOfNeurons.append(temp)\n",
        "  W=[]\n",
        "  b=[]\n",
        "  if weight_init == 'random':\n",
        "    W,b = Initalize_Wb(num_layers,numberOfNeurons)\n",
        "  else:\n",
        "    W,b = xavier_init(num_layers, numberOfNeurons)\n",
        "\n",
        "  if optimizer == 'GD':\n",
        "      W,b =gradient_descent(epochs,x_trainFlat,Y_actual, y_train,learning_rate,numberOfNeurons,num_layers,activation,W,b)\n",
        "      prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer =='sgd':\n",
        "    W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate, batch_size,W,b,numberOfNeurons,num_layers,activation)\n",
        "    prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'momentum':\n",
        "    W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,momentum,W,b,numberOfNeurons,num_layers,activation)\n",
        "    prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'nestrov':\n",
        "    W,b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,momentum,W,b,numberOfNeurons,num_layers,activation)\n",
        "    prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'rmsprop':\n",
        "     W,b = rmsprop(x_trainFlat, Y_actual, y_train,epochs, learning_rate, batch_size,beta, epsilon,W,b,numberOfNeurons,num_layers,activation)\n",
        "     prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "  elif optimizer == 'adam':\n",
        "     W,b = adam(x_trainFlat, Y_actual, y_train,epochs,learning_rate,batch_size,beta1,beta2,epsilon,W,b,numberOfNeurons,num_layers,activation)\n",
        "     prediction_Test(x_testFlat,W,b,num_layers,activation)\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "neuralNetwork(epochs=11,batch_size= 64,optimizer ='adam',learning_rate = 0.1,beta =0.01,beta1 = 0.9,beta2 =0.99,epsilon =1e-3,weight_init='random',num_layers=3,hidden_size=64,activation='ReLU',momentum=0.9)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jeeeavu4pvhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590bc0b0-ed29-499b-8ede-58abe5714ef0"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 2 9 ... 2 0 1] [5 2 9 ... 6 6 1]\n",
            "Iteration 0: accuracy = 83.53%\n",
            "[7 2 9 ... 2 2 1] [5 2 9 ... 6 6 1]\n",
            "Iteration 5: accuracy = 86.07%\n",
            "[5 2 9 ... 2 0 1] [5 2 9 ... 6 6 1]\n",
            "Iteration 10: accuracy = 86.61%\n",
            "[9 2 1 ... 8 1 5] [9 2 1 ... 8 1 5]\n",
            "Test_Accuracy = 83.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CysxuvIRv6za"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run=wandb.init(config=default_params,project='DLassignment1',entity='singhbhavesh999',reinit='true')\n",
        "# config=wandb.config"
      ],
      "metadata": {
        "id": "ia82xGBa7cHW"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5dk0_OMzqM6_"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep_config={\n",
        "#     'method' : 'bayes' ,\n",
        "#     'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\n",
        "#     'parameters' : {\n",
        "#         'epochs' : { 'values' : [5,10,20] },\n",
        "#         'n_hidden_layers' : {'values' : [1,2,3]},\n",
        "#         'n_hidden_layer_size' : { 'values' : [16,32,64,128,256]},\n",
        "#         'batch_size' : { 'values' : [16,32,64,128]},\n",
        "#         'learning_rate' : { 'values' : [0.1, 0.01,0.0001,0.0003]},\n",
        "#         'optimizer' : { 'values' : [\"sgd\", \"mgd\"] },\n",
        "#         'activations' : { 'values' : [\"sigmoid\", \"tanh\", \"relu\"] },\n",
        "#         #'loss_function' : {'values' : ['cross_entropy']},\n",
        "#         'weight_ini' : {'values' : ['random']},\n",
        "#        # 'weight_decay' : { 'values' : [0,0.0005]},\n",
        "#         'beta': {'values' : [0.5]},\n",
        "#         'beta1': {'values' : [0.5]},\n",
        "#         'beta2': {'values' : [0.5]},\n",
        "#         'momentum': {'values' : [0.5]},\n",
        "#         'epsilon' : {'values' : [1e-2]}\n",
        "\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# def tr():\n",
        "#   config_default={\n",
        "#       'weight_ini':'bayes',\n",
        "#       'n_hidden_layers':3,\n",
        "#       'n_hidden_layer_size':32,\n",
        "#       'optimizer':'sgd',\n",
        "#       'learning_rate':0.01,\n",
        "#       'epoch':10,\n",
        "#       'batch_size':32\n",
        "#   }\n",
        "#   wandb.init(config=config_default)\n",
        "\n",
        "#   c= wandb.config\n",
        "#   name = \"op_\"+str(c.optimizer)+\"ac\"+str(c.activations)+\"hl\"+str(c.n_hidden_layers)+\"hls\"+str(c.n_hidden_layer_size)+\"ep\"+str(c.epochs)+\"n\"+str(c.learning_rate)+\"bs\"+str(c.batch_size)+\"wi\"+str(c.weight_ini)\n",
        "#   wandb.init(name=name)\n",
        "\n",
        "#   hn = c.n_hidden_layer_size\n",
        "#   hl = c.n_hidden_layers\n",
        "#   act = c.activations\n",
        "#   opt = c.optimizer\n",
        "#   ep = c.epochs\n",
        "#   bs = c.batch_size\n",
        "#   lr = c.learning_rate\n",
        "#   wi = c.weight_ini\n",
        "#   mm =c.momentum\n",
        "#   b =c.beta\n",
        "#   b1=c.beta1\n",
        "#   b2=c.beta2\n",
        "#   e =c.epsilon\n",
        "#   neuralNetwork(ep,bs,opt, lr,b,b1,b2,e,wi,hl,hn,act,mm)\n",
        "#   return\n",
        "# sweep_id = wandb.sweep(sweep_config, project=\"dl_assignment\")\n",
        "# wandb.agent(sweep_id, function=tr,count=3)"
      ],
      "metadata": {
        "id": "Nir4rvaNqIkY"
      },
      "execution_count": 129,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvinashKushwah/cs6910_assignment1/blob/main/Question3_trail1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_oMtPmCeS3",
        "outputId": "6125af70-de00-4890-c04b-c5337bd64456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(y_train.shape)\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5eb7NvCAbX",
        "outputId": "6a767f8a-7936-4b17-af14-c72c2fe2481f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Normalize the image pixel intesity within a range of 0-1\n",
        "x_trainFlat = x_train.reshape(len(x_train),784).astype('float32')/255\n",
        "print(x_trainFlat.shape)\n",
        "x_testFlat = x_test.reshape(len(x_test),784).astype('float32')/255\n",
        "print(x_testFlat.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "id": "XuBTUkAWG_Km"
      },
      "outputs": [],
      "source": [
        "# Activation Function\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x in a numerically stable way.\n",
        "    \"\"\"\n",
        "    # Avoid overflow by clipping large values\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# def sigmoid(x):# limit the range of x to avoid overflow\n",
        "#   return 1 / (1 + np.exp(-x))\n",
        " \n",
        "def sigmoid_diff(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def ReLU_deriv(x):\n",
        "    return x > 0\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_deriv(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "def softmax1(Z):\n",
        "  A = np.exp(Z) / (sum(np.exp(Z)) + 1e-9)\n",
        "  return A\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ZYFHeQIy05",
        "outputId": "9725be2e-b838-4938-8778-9e6e2500a752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter number of Hidden layers:2\n"
          ]
        }
      ],
      "source": [
        "layers = int(input(\"enter number of Hidden layers:\"))\n",
        "layers = int(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "id": "5FC3bkmu2bs6"
      },
      "outputs": [],
      "source": [
        "# layers = input(\"enter number of Hidden layers:\")\n",
        "# layers = int(layers)\n",
        "# numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "# W = []                            #store the Weight matrix at each layer\n",
        "# b = []                            #store the bias vector at each layer\n",
        "\n",
        "\n",
        "# for i in range(layers):                       \n",
        "#   randomNumber = np.random.randint(4,10)\n",
        "#   numberOfNeurons.append(randomNumber)\n",
        "#   if( i== 0):\n",
        "#     w_temp =np.random.rand(randomNumber,784)\n",
        "#     b_temp = np.random.rand(randomNumber)\n",
        "#     W.append(w_temp)\n",
        "#     b.append(b_temp)\n",
        "#   else:\n",
        "#     w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])\n",
        "#     b_temp = np.random.rand(numberOfNeurons[i])\n",
        "#     W.append(w_temp);\n",
        "#     b.append(b_temp)\n",
        "\n",
        "# output_w = np.random.rand(10,numberOfNeurons[layers-1])\n",
        "# output_b = np.random.rand(10)\n",
        "# W.append(output_w)\n",
        "# b.append(output_b)\n",
        "\n",
        "# #print(len(W))\n",
        "# #print(len(b))\n",
        "# #print(W[0].shape)\n",
        "# # def randomIntialization(dimx,dimy):\n",
        "# #   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "# #   b_temp = np.random.randint(10,size=(dimx))\n",
        "# #   return W_temp, b_temp\n",
        "\n",
        "\n",
        "\n",
        "# def feedForward(input_data):\n",
        "    \n",
        "#    Y= sigmoid(np.dot(W[0],input_data) + b[0])\n",
        "#    for i in range(1,layers):\n",
        "#       Y = sigmoid(np.dot(W[i],Y)) + b[i]\n",
        "  \n",
        "#    output = sigmoid(np.dot(W[layers],Y)) + b[layers]\n",
        "#    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# output = feedForward(x_trainFlat[0])\n",
        "# outputSoftmax = softmax(output)\n",
        "# #print(output.shape)\n",
        "# print(outputSoftmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {
        "id": "6pl3IeUQQ6cC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47297f6-e634-4d05-d64c-9393ad67ba06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of neurons:128\n",
            "number of neurons:256\n"
          ]
        }
      ],
      "source": [
        "numberOfNeurons =[] \n",
        "for i in range(layers):\n",
        "  temp = int(input(\"number of neurons:\"))\n",
        "  numberOfNeurons.append(temp)\n",
        "def Initalize_Wb():\n",
        "  #layers = int(input(\"enter number of Hidden layers:\"))\n",
        "  #layers = int(layers)\n",
        " # numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "  W = []                            #store the Weight matrix at each layer\n",
        "  b = []                            #store the bias vector at each layer\n",
        "  for i in range(layers):                       \n",
        "    #randomNumber = np.random.randint(64,256)\n",
        "   # numberOfNeurons.append(randomNumber)\n",
        "    if( i== 0):\n",
        "      w_temp =np.random.rand(numberOfNeurons[i],784)-0.5\n",
        "      b_temp = np.random.rand(numberOfNeurons[i],1)-0.5\n",
        "      W.append(w_temp)\n",
        "      b.append(b_temp)\n",
        "    else:\n",
        "      w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])-0.5\n",
        "      b_temp = np.random.rand(numberOfNeurons[i],1)-0.5\n",
        "      W.append(w_temp);\n",
        "      b.append(b_temp)\n",
        "\n",
        "  print(numberOfNeurons)\n",
        "  output_w = np.random.rand(10,numberOfNeurons[layers-1])-0.5\n",
        "  output_b = np.random.rand(10,1)-0.5\n",
        "  W.append(output_w)\n",
        "  b.append(output_b)\n",
        "  # print(len(W))\n",
        "  # print(len(b))\n",
        "  # print(W[0].shape)\n",
        "  # print(W[1].shape)\n",
        "  # print(W[2].shape)\n",
        "  # print(b[0].shape)\n",
        "  # print(b[1].shape)\n",
        "  # print(b[2].shape)\n",
        "\n",
        "  return W,b\n",
        "\n",
        "\n",
        "\n",
        "def feedForward(input_data,W,b):\n",
        "  \n",
        "   output =[]\n",
        "   temp =[]\n",
        "   A =[]\n",
        "  #  print(\"hello\",b[0].shape)\n",
        "  #  print(b[0].reshape(-1,1).shape)\n",
        "   Y =np.dot(W[0],input_data) + b[0]\n",
        "   temp.append(Y)\n",
        "   Y= ReLU(Y)\n",
        "   A.append(Y)\n",
        "   for i in range(1,layers):\n",
        "      Y =np.dot(W[i],Y) + b[i]\n",
        "      temp.append(Y)\n",
        "      Y=ReLU(Y)\n",
        "      A.append(Y)\n",
        "   temp2=np.dot(W[layers],Y) + b[layers]\n",
        "   temp.append(temp2)\n",
        "   output=(softmax1(ReLU(temp2)))\n",
        "   #print(\"softmaxt\",output[:,1])\n",
        "   return output,temp,A\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def Initalize_Wb():\n",
        "#   #layers = int(input(\"enter number of Hidden layers:\"))\n",
        "#   #layers = int(layers)\n",
        "#   numberOfNeurons              # number of neurons at each Hidden Layer\n",
        "#   W = []                            #store the Weight matrix at each layer\n",
        "#   b = []                            #store the bias vector at each layer\n",
        "#   for i in range(layers):                       \n",
        "#     randomNumber = np.random.randint(64,256)\n",
        "#     numberOfNeurons.append(randomNumber)\n",
        "#     if( i== 0):\n",
        "#       w_temp =np.random.rand(randomNumber,784)-0.5\n",
        "#       b_temp = np.random.rand(randomNumber,1)-0.5\n",
        "#       W.append(w_temp)\n",
        "#       b.append(b_temp)\n",
        "#     else:\n",
        "#       w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])-0.5\n",
        "#       b_temp = np.random.rand(numberOfNeurons[i],1)-0.5\n",
        "#       W.append(w_temp);\n",
        "#       b.append(b_temp)\n",
        "\n",
        "#   print(numberOfNeurons)\n",
        "#   output_w = np.random.rand(10,numberOfNeurons[layers-1])-0.5\n",
        "#   output_b = np.random.rand(10,1)-0.5\n",
        "#   W.append(output_w)\n",
        "#   b.append(output_b)\n",
        "#   # print(len(W))\n",
        "#   # print(len(b))\n",
        "#   # print(W[0].shape)\n",
        "#   # print(W[1].shape)\n",
        "#   # print(W[2].shape)\n",
        "#   # print(b[0].shape)\n",
        "#   # print(b[1].shape)\n",
        "#   # print(b[2].shape)\n",
        "\n",
        "#   return W,b\n",
        "\n",
        "\n",
        "\n",
        "# def feedForward(input_data,W,b):\n",
        "  \n",
        "#    output =[]\n",
        "#    temp =[]\n",
        "#    A =[]\n",
        "#   #  print(\"hello\",b[0].shape)\n",
        "#   #  print(b[0].reshape(-1,1).shape)\n",
        "#    Y =np.dot(W[0],input_data) + b[0]\n",
        "#    temp.append(Y)\n",
        "#    Y= ReLU(Y)\n",
        "#    A.append(Y)\n",
        "#    for i in range(1,layers):\n",
        "#       Y =np.dot(W[i],Y) + b[i]\n",
        "#       temp.append(Y)\n",
        "#       Y=ReLU(Y)\n",
        "#       A.append(Y)\n",
        "#    temp2=np.dot(W[layers],Y) + b[layers]\n",
        "#    temp.append(temp2)\n",
        "#    output=(softmax1(ReLU(temp2)))\n",
        "#    #print(\"softmaxt\",output[:,1])\n",
        "#    return output,temp,A\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YogYFeJcE988"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmRsw01N1gS",
        "outputId": "7bdff6a7-3e25-48ef-ae62-94fadbf380c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 60000)\n"
          ]
        }
      ],
      "source": [
        "def find_Actuall_Y():\n",
        "  X,Y = x_trainFlat.shape\n",
        "  # print(X)\n",
        "  Y_actual=[]\n",
        "  for i in range(X):\n",
        "    tempX =x_trainFlat[i]\n",
        "    tempY = y_train[i]\n",
        "    vector = np.zeros(10)             # creating a vector of size 10 and assigning '1' to the correct index of trainFlat[i]\n",
        "    vector[tempY] =1\n",
        "    Y_actual.append(vector)\n",
        "    # #outputY=softmax(feedForward(x_trainFlat[i]))\n",
        "    # Y_predicted.append(outputY)\n",
        "    # loss = -np.mean(np.sum(Y_actual * np.log(Y_predicted),axis =1))          #crossEntropy loss Caluclation\n",
        "  return Y_actual\n",
        "\n",
        "\n",
        "Y_actual = find_Actuall_Y()\n",
        "Y_actual = np.array(Y_actual)\n",
        "Y_actual = Y_actual.T\n",
        "print(Y_actual.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Assuming Y_predicted and Y_actual are 10x60000 arrays\n",
        "def cross_entropy_loss(Y_actual, Y_predicted):\n",
        "    # Clip predicted values to avoid log(0) numerical instability\n",
        "    Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Compute cross-entropy loss\n",
        "    N = Y_actual.shape[1] # number of samples\n",
        "    loss = -1/N * np.sum(Y_actual * np.log(Y_predicted))\n",
        "    return loss\n",
        "\n",
        "# error = cross_entropy_loss(Y_actual, Y_predicted)\n",
        "# print(error)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "id": "dcBVLnv5ftzg"
      },
      "outputs": [],
      "source": [
        "#Backpropogation Function\n",
        "\n",
        "def backpropagation(X, Y_actual, Y_predicted, Intermediate_Activation,A,W,b):\n",
        "    dW = [None] * (len(W)) # gradient of weight matrix at each layer\n",
        "    db = [None] * (len(b)) # gradient of bias vector at each layer\n",
        "    \n",
        "    # Compute derivative of loss with respect to output\n",
        "    dL_dZ = Y_predicted - Y_actual\n",
        "    \n",
        "    # Compute gradients for output layer\n",
        "   #dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[-1].T)\n",
        "    dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, A[-1].T)\n",
        "    db[-1] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "   # print(dL_dZ.shape)\n",
        "    #print(db[-1].shape)\n",
        "    \n",
        "    # Backpropagate gradients through hidden layers\n",
        "    for l in reversed(range(1, len(W)-1)):\n",
        "        dL_dY = np.dot(W[l+1].T, dL_dZ)\n",
        "        # print(dL_dY.shape)\n",
        "        # print(Intermediate_Activation[l].shape)\n",
        "        # zy= sigmoid_diff(Intermediate_Activation[l-1])\n",
        "        # print(zy.shape)\n",
        "        dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        dW[l] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[l-1].T)\n",
        "        db[l] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "\n",
        "    \n",
        "    # Compute gradients for input layer\n",
        "    dL_dY = np.dot(W[1].T, dL_dZ)\n",
        "    dL_dZ = dL_dY * sigmoid_diff(Intermediate_Activation[0])\n",
        "    dW[0] = 1/X.shape[0] * np.dot(dL_dZ, X)\n",
        "    db[0] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "    # print(\"insside update\",len(db))\n",
        "    # print(db[0].shape)\n",
        "    # print(db[1].shape)\n",
        "    # print(db[2].shape)\n",
        "    return dW, db\n",
        "\n",
        "# dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "# print(dW[0].shape,db[0].shape)\n",
        "# print(dW[1].shape,db[1].shape)\n",
        "# print(dW[2].shape,db[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "id": "Demu45MGTSNp"
      },
      "outputs": [],
      "source": [
        "def Update(W,b,dW,db,learningRate):\n",
        "\n",
        "  for i in range (len(W)):\n",
        "    W[i] = W[i] -learningRate*dW[i]\n",
        "   # print(\"before update\",b[i].shape,db[i].shape)\n",
        "    b[i] = b[i] -learningRate* db[i]\n",
        "    #print(\"after update\",b[i].shape, W[i].shape)\n",
        "  \n",
        "  return W,b\n",
        "\n",
        "\n",
        "#W,b=Update(W,b,dW,db,0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "XaLQM-cQelX4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "OPeNQ0y-jijy"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x_trainFlat,Y_actual,y_train,iteration,learning_rate):\n",
        "    W,b = Initalize_Wb()\n",
        "    print(len(W),len(b))\n",
        "    for i in range(iteration):\n",
        "        Y_predicted, Intermediate_Activation, A = feedForward(x_trainFlat.T,W,b)\n",
        "        dW, db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation,A,W,b)\n",
        "        # print(\"x\",len(b),b[0].shape,b[1].shape,b[2].shape)\n",
        "        W,b = Update(W,b,dW,db,learning_rate)\n",
        "        #print(\"len of b\",len(b))\n",
        "        if i % 10 == 0:\n",
        "            print(\"iteration:\", i)\n",
        "           # y_pred = get_prediction(Y_predicted)\n",
        "            #print(\"Ypredicted\",Y_predicted[:,1])\n",
        "            #print(\"y_actual\",Y_actual[:,1])\n",
        "            #acuracy = getAccuracy(Y_predicted, Y_actual)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            print(\"Accuracy:\", get_accuracy(predictions,y_train)*100)\n",
        "    return W,b\n",
        "     \n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat, Y_actual,y_train,1000, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "6hQ5LsDPX5AP"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size):\n",
        "    W, b = Initalize_Wb()\n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            # Update the parameters using the batch gradient\n",
        "            W, b = Update(W, b, dW_batch, db_batch, learning_rate)\n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    return W, b\n",
        "\n",
        "\n",
        "#W,b = stochastic_gradient_descent(x_trainFlat,Y_actual,y_train,100,0.1,64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {
        "id": "AdMYsitfTcIg"
      },
      "outputs": [],
      "source": [
        "def momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, momentum):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # print(len(v_dW))\n",
        "    # print(len(v_db))\n",
        "    \n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [momentum * v_db + (1-momentum) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n",
        "#W, b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=1000, learning_rate=0.1,batch_size=64, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "id": "EUpuizVXkVFN"
      },
      "outputs": [],
      "source": [
        "def nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, momentum,batch_size):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # print(len(v_dW))\n",
        "    # print(len(v_db))\n",
        "    \n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "\n",
        "            # Compute the gradients at the lookahead position\n",
        "            W_lookahead = [w - momentum * v_dw for w, v_dw in zip(W, v_dW)]\n",
        "            b_lookahead = [b - momentum * v_db for b, v_db in zip(b, v_db)]\n",
        "            Y_predicted_lookahead, _, _ = feedForward(x_batch.T, W_lookahead, b_lookahead)\n",
        "            dW_batch_lookahead, db_batch_lookahead = backpropagation(x_batch, Y_actual_batch, Y_predicted_lookahead, Intermediate_Activation_batch, A_batch, W_lookahead, b_lookahead)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch_lookahead)]\n",
        "            v_db = [momentum * v_db + (1-momentum) * db for v_db, db in zip(v_db, db_batch_lookahead)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "id": "JFdotIUVmkRJ"
      },
      "outputs": [],
      "source": [
        "def rmsprop(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, beta,epsilon):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize accumulated gradients to zero\n",
        "    s_dW = [np.zeros_like(w) for w in W]\n",
        "    s_db = [np.zeros_like(b) for b in b]\n",
        "\n",
        "    # set a small constant to avoid division by zero\n",
        "    #epsilon = 1e-8\n",
        "    \n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            \n",
        "            # Accumulate the squared gradients\n",
        "            s_dW = [beta * s_dw + (1-beta) * dw**2 for s_dw, dw in zip(s_dW, dW_batch)]\n",
        "            s_db = [beta * s_db + (1-beta) * db**2 for s_db, db in zip(s_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the accumulated gradients\n",
        "            W = [w - learning_rate * dw / np.sqrt(s_dw + epsilon) for w, dw, s_dw in zip(W, dW_batch, s_dW)]\n",
        "            b = [b - learning_rate * db / np.sqrt(s_db + epsilon) for b, db, s_db in zip(b, db_batch, s_db)]\n",
        "            \n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "id": "9oopKx7uTi-6"
      },
      "outputs": [],
      "source": [
        "def prediction_Test(input_data,W,b):\n",
        "  Y_predicted, _, _ = feedForward(input_data.T, W, b)\n",
        "  predictions = get_predictions(Y_predicted)\n",
        "  accuracy = get_accuracy(predictions, y_test) * 100\n",
        "  print(f\"Test_Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {
        "id": "EpMQOAqQiCxz"
      },
      "outputs": [],
      "source": [
        "# user_input = input(\"Enter A for gradient descent, B for stochastic gradient descent, C for momentum gradient descent, or D for Nesterov gradient descent: \")\n",
        "\n",
        "# if user_input == \"A\":\n",
        "#     W,b =gradient_descent(x_trainFlat, Y_actual, y_train, iteration=50, learning_rate=0.1)\n",
        "#     prediction_Test(x_testFlat,W,b)\n",
        "# elif user_input == \"B\":\n",
        "#     W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=60, learning_rate=0.1, batch_size=64)\n",
        "#     prediction_Test(x_testFlat,W,b)\n",
        "# elif user_input == \"C\":\n",
        "#     W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=50, learning_rate=0.1, batch_size=64, momentum=0.9)\n",
        "#     prediction_Test(x_testFlat,W,b)\n",
        "# elif user_input == \"D\":\n",
        "    \n",
        "    \n",
        "# elif user_input == \"E\":\n",
        "#     W,b = rmsprop(x_trainFlat, Y_actual, y_train, iteration=50, learning_rate=0.01, batch_size=64, decay_rate=0.9, epsilon=1e-8)\n",
        "\n",
        "\n",
        "# else:\n",
        "#     print(\"Invalid input. Please enter A, B, C, or D.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GD\")\n",
        "W,b =gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1)\n",
        "prediction_Test(x_testFlat,W,b)\n",
        "print(\"sGD\")\n",
        "W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64)\n",
        "prediction_Test(x_testFlat,W,b)\n",
        "print(\"MGD\")\n",
        "W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, batch_size=64, momentum=0.9)\n",
        "prediction_Test(x_testFlat,W,b)\n",
        "print(\"NGD\")\n",
        "W,b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.1, momentum=0.9, batch_size=64)\n",
        "prediction_Test(x_testFlat,W,b)\n",
        "print(\"rms\")\n",
        "W,b = rmsprop(x_trainFlat, Y_actual, y_train, iteration=10, learning_rate=0.01, batch_size=64, beta=0.9, epsilon=1e-8)\n",
        "prediction_Test(x_testFlat,W,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFo3Ug_dBWY9",
        "outputId": "aea4bf3f-394c-4f1d-d030-c45fd03e34a0"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GD\n",
            "[128, 256]\n",
            "3 3\n",
            "iteration: 0\n",
            "[7 5 5 ... 5 5 6] [9 0 0 ... 3 0 5]\n",
            "Accuracy: 7.0216666666666665\n",
            "[9 2 1 ... 3 3 7] [9 2 1 ... 8 1 5]\n",
            "Test_Accuracy = 59.31%\n",
            "sGD\n",
            "[128, 256]\n",
            "[9 0 3 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 77.50%\n",
            "[9 2 1 ... 8 1 5] [9 2 1 ... 8 1 5]\n",
            "Test_Accuracy = 83.68%\n",
            "MGD\n",
            "[128, 256]\n",
            "[9 0 3 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 81.84%\n",
            "[9 2 1 ... 8 1 5] [9 2 1 ... 8 1 5]\n",
            "Test_Accuracy = 84.66%\n",
            "NGD\n",
            "[128, 256]\n",
            "[9 0 3 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 81.38%\n",
            "[9 2 1 ... 8 1 7] [9 2 1 ... 8 1 5]\n",
            "Test_Accuracy = 84.16%\n",
            "rms\n",
            "[128, 256]\n",
            "[9 0 3 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 74.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-422-06c0b4e6568f>:31: RuntimeWarning: overflow encountered in exp\n",
            "  A = np.exp(Z) / (sum(np.exp(Z)) + 1e-9)\n",
            "<ipython-input-422-06c0b4e6568f>:31: RuntimeWarning: invalid value encountered in true_divide\n",
            "  A = np.exp(Z) / (sum(np.exp(Z)) + 1e-9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0] [9 2 1 ... 8 1 5]\n",
            "Test_Accuracy = 10.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
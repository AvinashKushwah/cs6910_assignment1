{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_oMtPmCeS3",
        "outputId": "2326ea7d-9cc7-45ee-c64e-18bfa27c6400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "[9 0 0 ... 3 0 5]\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(y_train)\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5eb7NvCAbX",
        "outputId": "a5aa2810-7c1c-4d6d-8bad-f2d87baaafe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "# Normalize the image pixel intesity within a range of 0-1\n",
        "x_trainFlat = x_train.reshape(len(x_train),784).astype('float32')/255\n",
        "print(x_trainFlat.shape)\n",
        "x_testFlat = x_test.reshape(len(x_test),784).astype('float32')/255\n",
        "print(x_trainFlat.shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuBTUkAWG_Km"
      },
      "outputs": [],
      "source": [
        "# Activation Function\n",
        "\n",
        "def sigmoid(x):# limit the range of x to avoid overflow\n",
        "  return 1 / (1 + np.exp(-x))\n",
        " \n",
        "def sigmoid_diff(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "# def softmax(x):\n",
        "#   # x -= np.max(x,axis =1,keepdims=True)  #subtracting to avoid numerical instablity\n",
        "#   # temp = np.exp(x)\n",
        "#   # softmax_result = temp/np.sum(temp,axis=1,keepdims=True)\n",
        "#   # return softmax_result\n",
        "#   x = x.reshape((1,10))\n",
        "#   exp_x = np.exp(x)\n",
        "#   return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ZYFHeQIy05",
        "outputId": "d9795644-0fa7-42fe-cb90-4a23fbab4e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter number of Hidden layers:3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # x_train = x_train/255\n",
        "# # x_test = x_test /255\n",
        "# print(y_train[0])\n",
        "# print(x_train[0].shape)\n",
        "# plt.matshow(x_train[0])\n",
        "layers = int(input(\"enter number of Hidden layers:\"))\n",
        "layers = int(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FC3bkmu2bs6"
      },
      "outputs": [],
      "source": [
        "# layers = input(\"enter number of Hidden layers:\")\n",
        "# layers = int(layers)\n",
        "# numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "# W = []                            #store the Weight matrix at each layer\n",
        "# b = []                            #store the bias vector at each layer\n",
        "\n",
        "\n",
        "# for i in range(layers):                       \n",
        "#   randomNumber = np.random.randint(4,10)\n",
        "#   numberOfNeurons.append(randomNumber)\n",
        "#   if( i== 0):\n",
        "#     w_temp =np.random.rand(randomNumber,784)\n",
        "#     b_temp = np.random.rand(randomNumber)\n",
        "#     W.append(w_temp)\n",
        "#     b.append(b_temp)\n",
        "#   else:\n",
        "#     w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])\n",
        "#     b_temp = np.random.rand(numberOfNeurons[i])\n",
        "#     W.append(w_temp);\n",
        "#     b.append(b_temp)\n",
        "\n",
        "# output_w = np.random.rand(10,numberOfNeurons[layers-1])\n",
        "# output_b = np.random.rand(10)\n",
        "# W.append(output_w)\n",
        "# b.append(output_b)\n",
        "\n",
        "# #print(len(W))\n",
        "# #print(len(b))\n",
        "# #print(W[0].shape)\n",
        "# # def randomIntialization(dimx,dimy):\n",
        "# #   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "# #   b_temp = np.random.randint(10,size=(dimx))\n",
        "# #   return W_temp, b_temp\n",
        "\n",
        "\n",
        "\n",
        "# def feedForward(input_data):\n",
        "    \n",
        "#    Y= sigmoid(np.dot(W[0],input_data) + b[0])\n",
        "#    for i in range(1,layers):\n",
        "#       Y = sigmoid(np.dot(W[i],Y)) + b[i]\n",
        "  \n",
        "#    output = sigmoid(np.dot(W[layers],Y)) + b[layers]\n",
        "#    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# output = feedForward(x_trainFlat[0])\n",
        "# outputSoftmax = softmax(output)\n",
        "# #print(output.shape)\n",
        "# print(outputSoftmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pl3IeUQQ6cC"
      },
      "outputs": [],
      "source": [
        "# def softmax1(x):\n",
        "#   x -= np.max(x,axis =1,keepdims=True)  #subtracting to avoid numerical instablity\n",
        "#   temp = np.exp(x)\n",
        "#   softmax_result = temp/np.sum(temp,axis=1,keepdims=True)\n",
        "#   return softmax_result\n",
        " # x = x.reshape((1,10))\n",
        "  # exp_x = np.exp(x)\n",
        "  # return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def softmax1(Z):\n",
        "\n",
        "  A = np.exp(Z) / sum(np.exp(Z))\n",
        "  return A\n",
        "  \n",
        "def ReLU(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def ReLU_deriv(x):\n",
        "    return x > 0\n",
        "\n",
        "def Initalize_Wb():\n",
        "  #layers = int(input(\"enter number of Hidden layers:\"))\n",
        "  #layers = int(layers)\n",
        "  numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "  W = []                            #store the Weight matrix at each layer\n",
        "  b = []                            #store the bias vector at each layer\n",
        "  for i in range(layers):                       \n",
        "    randomNumber = np.random.randint(64,256)\n",
        "    numberOfNeurons.append(randomNumber)\n",
        "    if( i== 0):\n",
        "      w_temp =np.random.rand(randomNumber,784)-0.5\n",
        "      b_temp = np.random.rand(randomNumber,1)-0.5\n",
        "      W.append(w_temp)\n",
        "      b.append(b_temp)\n",
        "    else:\n",
        "      w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])-0.5\n",
        "      b_temp = np.random.rand(numberOfNeurons[i],1)-0.5\n",
        "      W.append(w_temp);\n",
        "      b.append(b_temp)\n",
        "\n",
        "  print(numberOfNeurons)\n",
        "  output_w = np.random.rand(10,numberOfNeurons[layers-1])-0.5\n",
        "  output_b = np.random.rand(10,1)-0.5\n",
        "  W.append(output_w)\n",
        "  b.append(output_b)\n",
        "  print(len(W))\n",
        "  print(len(b))\n",
        "  print(W[0].shape)\n",
        "  print(W[1].shape)\n",
        "  print(W[2].shape)\n",
        "  print(b[0].shape)\n",
        "  print(b[1].shape)\n",
        "  print(b[2].shape)\n",
        "\n",
        "  return W,b\n",
        "\n",
        "\n",
        "# def randomIntialization(dimx,dimy):\n",
        "#   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "#   b_temp = np.random.randint(10,size=(dimx))\n",
        "#   return W_temp, b_temp\n",
        "\n",
        "def feedForward(input_data,W,b):\n",
        "  \n",
        "   output =[]\n",
        "   temp =[]\n",
        "   A =[]\n",
        "  #  print(\"hello\",b[0].shape)\n",
        "  #  print(b[0].reshape(-1,1).shape)\n",
        "   Y =np.dot(W[0],input_data) + b[0]\n",
        "   temp.append(Y)\n",
        "   Y= ReLU(Y)\n",
        "   A.append(Y)\n",
        "   for i in range(1,layers):\n",
        "      Y =np.dot(W[i],Y) + b[i]\n",
        "      temp.append(Y)\n",
        "      Y=ReLU(Y)\n",
        "      A.append(Y)\n",
        "   temp2=np.dot(W[layers],Y) + b[layers]\n",
        "   temp.append(temp2)\n",
        "   output=(softmax1(ReLU(temp2)))\n",
        "   #print(\"softmaxt\",output[:,1])\n",
        "   return output,temp,A\n",
        "\n",
        "\n",
        "\n",
        "#print(x_trainFlat.shape)\n",
        "#Y_predicted,Intermediate_Activation,A= feedForward(x_trainFlat.T)\n",
        "#Y_predicted = np.array(Y_predicted)\n",
        "#Intermediate_Activation = np.array(Intermediate_Activation)\n",
        "#print(Y_predicted[0][1])\n",
        "# print(Y_predicted.shape)\n",
        "# print(Intermediate_Activation[0].shape)\n",
        "# print(Intermediate_Activation[1].shape)\n",
        "# print(Intermediate_Activation[2].shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmRsw01N1gS",
        "outputId": "edad40fa-7e38-4393-e68d-d06272a6d517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 60000)\n"
          ]
        }
      ],
      "source": [
        "def find_Actuall_Y():\n",
        "  X,Y = x_trainFlat.shape\n",
        "  # print(X)\n",
        "  Y_actual=[]\n",
        "  for i in range(X):\n",
        "    tempX =x_trainFlat[i]\n",
        "    tempY = y_train[i]\n",
        "    vector = np.zeros(10)             # creating a vector of size 10 and assigning '1' to the correct index of trainFlat[i]\n",
        "    vector[tempY] =1\n",
        "    Y_actual.append(vector)\n",
        "    # #outputY=softmax(feedForward(x_trainFlat[i]))\n",
        "    # Y_predicted.append(outputY)\n",
        "    # loss = -np.mean(np.sum(Y_actual * np.log(Y_predicted),axis =1))          #crossEntropy loss Caluclation\n",
        "  return Y_actual\n",
        "\n",
        "\n",
        "Y_actual = find_Actuall_Y()\n",
        "Y_actual = np.array(Y_actual)\n",
        "Y_actual = Y_actual.T\n",
        "print(Y_actual.shape)\n",
        "\n",
        "# def errorCalculation(Y_actual,Y_predicted):\n",
        "#   Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7) # you may want to clip them to avoid log 0. avoiding numercial instablity\n",
        "#   temp = Y_actual.shape[0]\n",
        "#   loss = -1/temp * np.sum(np.multiply(Y_actual, np.log(Y_predicted)) + np.multiply((1 - Y_actual), np.log(1 - Y_predicted)))\n",
        "\n",
        "\n",
        "# errorLoss = errorCalculation(Y_actual, Y_predicted)\n",
        "# print(errorLoss)\n",
        "\n",
        "# Assuming Y_predicted and Y_actual are 10x60000 arrays\n",
        "def cross_entropy_loss(Y_actual, Y_predicted):\n",
        "    # Clip predicted values to avoid log(0) numerical instability\n",
        "    Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Compute cross-entropy loss\n",
        "    N = Y_actual.shape[1] # number of samples\n",
        "    loss = -1/N * np.sum(Y_actual * np.log(Y_predicted))\n",
        "    return loss\n",
        "\n",
        "# error = cross_entropy_loss(Y_actual, Y_predicted)\n",
        "# print(error)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcBVLnv5ftzg"
      },
      "outputs": [],
      "source": [
        "#Backpropogation Function\n",
        "\n",
        "def backpropagation(X, Y_actual, Y_predicted, Intermediate_Activation,A,W,b):\n",
        "    dW = [None] * (len(W)) # gradient of weight matrix at each layer\n",
        "    db = [None] * (len(b)) # gradient of bias vector at each layer\n",
        "    \n",
        "    # Compute derivative of loss with respect to output\n",
        "    dL_dZ = Y_predicted - Y_actual\n",
        "    \n",
        "    # Compute gradients for output layer\n",
        "   #dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[-1].T)\n",
        "    dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, A[-1].T)\n",
        "    db[-1] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "   # print(dL_dZ.shape)\n",
        "    #print(db[-1].shape)\n",
        "    \n",
        "    # Backpropagate gradients through hidden layers\n",
        "    for l in reversed(range(1, len(W)-1)):\n",
        "        dL_dY = np.dot(W[l+1].T, dL_dZ)\n",
        "        # print(dL_dY.shape)\n",
        "        # print(Intermediate_Activation[l].shape)\n",
        "        # zy= sigmoid_diff(Intermediate_Activation[l-1])\n",
        "        # print(zy.shape)\n",
        "        dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        dW[l] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[l-1].T)\n",
        "        db[l] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "\n",
        "    \n",
        "    # Compute gradients for input layer\n",
        "    dL_dY = np.dot(W[1].T, dL_dZ)\n",
        "    dL_dZ = dL_dY * sigmoid_diff(Intermediate_Activation[0])\n",
        "    dW[0] = 1/X.shape[0] * np.dot(dL_dZ, X)\n",
        "    db[0] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "    # print(\"insside update\",len(db))\n",
        "    # print(db[0].shape)\n",
        "    # print(db[1].shape)\n",
        "    # print(db[2].shape)\n",
        "    return dW, db\n",
        "\n",
        "# dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "# print(dW[0].shape,db[0].shape)\n",
        "# print(dW[1].shape,db[1].shape)\n",
        "# print(dW[2].shape,db[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Demu45MGTSNp"
      },
      "outputs": [],
      "source": [
        "def Update(W,b,dW,db,learningRate):\n",
        "\n",
        "  for i in range (len(W)):\n",
        "    W[i] = W[i] -learningRate*dW[i]\n",
        "   # print(\"before update\",b[i].shape,db[i].shape)\n",
        "    b[i] = b[i] -learningRate* db[i]\n",
        "    #print(\"after update\",b[i].shape, W[i].shape)\n",
        "  \n",
        "  return W,b\n",
        "\n",
        "\n",
        "#W,b=Update(W,b,dW,db,0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaLQM-cQelX4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def gradient_descent(x_trainFlat,y_train,Y_actual,iteration,learning_rate,W,b):\n",
        "\n",
        "#   for i in range(iteration):\n",
        "#     Y_predicted,Intermediate_Activation,A= feedForward(x_trainFlat.T)\n",
        "#     Y_predicted = np.array(Y_predicted)\n",
        "#     dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "#     W,b = Update(W,b,dW,db,learning_rate)\n",
        "#     if(i%50 ==0):\n",
        "#       print(\"iteration:\",i)\n",
        "#       print(\"Accuracy:\",get_Accuracy(get_prediction(Y_predicted),y_train))\n",
        "    \n",
        "#     return W,b\n",
        "\n",
        "\n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat,Y_actual,1000,0.1,W,b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPeNQ0y-jijy"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x_trainFlat,Y_actual,y_train,iteration,learning_rate):\n",
        "    W,b = Initalize_Wb()\n",
        "    print(len(W),len(b))\n",
        "    for i in range(iteration):\n",
        "        Y_predicted, Intermediate_Activation, A = feedForward(x_trainFlat.T,W,b)\n",
        "        dW, db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation,A,W,b)\n",
        "        # print(\"x\",len(b),b[0].shape,b[1].shape,b[2].shape)\n",
        "        W,b = Update(W,b,dW,db,learning_rate)\n",
        "        #print(\"len of b\",len(b))\n",
        "        if i % 10 == 0:\n",
        "            print(\"iteration:\", i)\n",
        "           # y_pred = get_prediction(Y_predicted)\n",
        "            #print(\"Ypredicted\",Y_predicted[:,1])\n",
        "            #print(\"y_actual\",Y_actual[:,1])\n",
        "            #acuracy = getAccuracy(Y_predicted, Y_actual)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            print(\"Accuracy:\", get_accuracy(predictions,y_train)*100)\n",
        "    return W,b\n",
        "     \n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat, Y_actual,y_train,1000, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hQ5LsDPX5AP"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size):\n",
        "    W, b = Initalize_Wb()\n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            # Update the parameters using the batch gradient\n",
        "            W, b = Update(W, b, dW_batch, db_batch, learning_rate)\n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    return W, b\n",
        "\n",
        "\n",
        "#W,b = stochastic_gradient_descent(x_trainFlat,Y_actual,y_train,100,0.1,64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGYTSl6CrNEW"
      },
      "outputs": [],
      "source": [
        "def momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size, momentum):\n",
        "    W, b = Initalize_Wb()\n",
        "    # initialize velocities to zero\n",
        "    v_dW = [np.zeros_like(w) for w in W]\n",
        "    v_db = [np.zeros_like(b) for b in b]\n",
        "    \n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            \n",
        "            # Update velocities\n",
        "            v_dW = [momentum * v_dw + (1-momentum) * dw for v_dw, dw in zip(v_dW, dW_batch)]\n",
        "            v_db = [momentum * v_db + (1-momentum) * db for v_db, db in zip(v_db, db_batch)]\n",
        "            \n",
        "            # Update the parameters using the velocity\n",
        "            W, b = Update(W, b, v_dW, v_db, learning_rate)\n",
        "            \n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "    return W, b\n",
        "#W, b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=1000, learning_rate=0.1,batch_size=64, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpL_fv2MsbvG"
      },
      "outputs": [],
      "source": [
        "# def nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, momentum, batch_size):\n",
        "#     W, b = Initalize_Wb()\n",
        "#     v_dW = np.zeros_like(W)\n",
        "#     v_db = np.zeros_like(b)\n",
        "#     v_dW = np.array(v_dW)\n",
        "#     v_db = np.array(v_db) \n",
        "#     for i in range(iteration):\n",
        "#         # Randomly shuffle the training data and split it into batches\n",
        "#         idx = np.random.permutation(len(x_trainFlat))\n",
        "#         x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "#         Y_actual_shuffled = Y_actual[:, idx]\n",
        "#         y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "#         # Update the parameters using the momentum gradient\n",
        "#         W_lookahead = W + momentum * v_dW\n",
        "#         b_lookahead = b + momentum * v_db\n",
        "        \n",
        "#         for j in range(0, len(x_trainFlat), batch_size):\n",
        "#             # Select a batch of training examples\n",
        "#             x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "#             Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "#             y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "#             # Compute the forward pass and backpropagation for the batch\n",
        "#             Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W_lookahead, b_lookahead)\n",
        "#             dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W_lookahead, b_lookahead)\n",
        "            \n",
        "           \n",
        "#             # Update the momentum\n",
        "#             v_dW = momentum * v_dW - learning_rate * dW_batch\n",
        "#             v_db = momentum * v_db - learning_rate * db_batch\n",
        "            \n",
        "#             # Update the parameters using the momentum gradient and Nesterov update\n",
        "#             W = W_lookahead + momentum * v_dW - learning_rate * dW_batch\n",
        "#             b = b_lookahead + momentum * v_db - learning_rate * db_batch\n",
        "        \n",
        "#         if i % 10 == 0:\n",
        "#             # Compute the accuracy on the entire training set\n",
        "#             Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "#             predictions = get_predictions(Y_predicted)\n",
        "#             accuracy = get_accuracy(predictions, y_train) * 100\n",
        "#             print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "#     return W, b\n",
        "\n",
        "# W, b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=1000, learning_rate=0.1, momentum=0.9,batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AdMYsitfTcIg",
        "outputId": "52f698f5-ef56-4640-a77d-93a6e353be0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[185, 230]\n",
            "3\n",
            "3\n",
            "(185, 784)\n",
            "(230, 185)\n",
            "(10, 230)\n",
            "(185, 1)\n",
            "(230, 1)\n",
            "(10, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "<ipython-input-34-4cd0a703586f>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  W_lookahead = W + momentum * v_dW\n",
            "<ipython-input-34-4cd0a703586f>:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  b_lookahead = b + momentum * v_db\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4cd0a703586f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnesterov_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainFlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-4cd0a703586f>\u001b[0m in \u001b[0;36mnesterov_gradient_descent\u001b[0;34m(x_trainFlat, Y_actual, y_train, iteration, learning_rate, momentum, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Update the momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mv_dW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_dW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mv_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_db\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
          ]
        }
      ],
      "source": [
        "# def nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, momentum, batch_size):\n",
        "#     W, b = Initalize_Wb()\n",
        "#     v_dW = np.zeros_like(W)\n",
        "#     v_db = np.zeros_like(b)\n",
        "#     for i in range(iteration):\n",
        "#         # Randomly shuffle the training data and split it into batches\n",
        "#         idx = np.random.permutation(len(x_trainFlat))\n",
        "#         x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "#         Y_actual_shuffled = Y_actual[:, idx]\n",
        "#         y_train_shuffled = np.array(y_train)[idx]\n",
        "        \n",
        "#         # Update the parameters using the momentum gradient\n",
        "#         W_lookahead = W + momentum * v_dW\n",
        "#         b_lookahead = b + momentum * v_db\n",
        "        \n",
        "#         for j in range(0, len(x_trainFlat), batch_size):\n",
        "#             # Select a batch of training examples\n",
        "#             x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "#             Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "#             y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            \n",
        "#             # Compute the forward pass and backpropagation for the batch\n",
        "#             Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W_lookahead, b_lookahead)\n",
        "#             dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W_lookahead, b_lookahead)\n",
        "          \n",
        "#             # Update the momentum\n",
        "#             v_dW = momentum * v_dW - learning_rate * dW_batch\n",
        "#             v_db = momentum * v_db - learning_rate * db_batch\n",
        "            \n",
        "#             # Update the parameters using the momentum gradient and Nesterov update\n",
        "#             v_dW = np.array(v_dW) # convert v_dW to a numpy array\n",
        "#             v_db = np.array(v_db) # convert v_db to a numpy array\n",
        "#             W = W_lookahead + momentum * v_dW - learning_rate * dW_batch\n",
        "#             b = b_lookahead + momentum * v_db - learning_rate * db_batch\n",
        "        \n",
        "#         if i % 10 == 0:\n",
        "#             # Compute the accuracy on the entire training set\n",
        "#             Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "#             predictions = get_predictions(Y_predicted)\n",
        "#             accuracy = get_accuracy(predictions, y_train) * 100\n",
        "#             print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    \n",
        "#     return W, b\n",
        "\n",
        "# W, b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=100, learning_rate=0.1, momentum=0.9,batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpMQOAqQiCxz",
        "outputId": "048f1d1c-03b4-43d8-cd6b-3020297e1df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[171, 151]\n",
            "3\n",
            "3\n",
            "(171, 784)\n",
            "(151, 171)\n",
            "(10, 151)\n",
            "(171, 1)\n",
            "(151, 1)\n",
            "(10, 1)\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 81.73%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 10: accuracy = 87.36%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 20: accuracy = 88.35%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 30: accuracy = 88.61%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 40: accuracy = 90.22%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 50: accuracy = 91.28%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 60: accuracy = 91.62%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 70: accuracy = 92.09%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 80: accuracy = 92.79%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 90: accuracy = 92.64%\n"
          ]
        }
      ],
      "source": [
        "user_input = input(\"Enter A for gradient descent, B for stochastic gradient descent, C for momentum gradient descent, or D for Nesterov gradient descent: \")\n",
        "\n",
        "if user_input == \"A\":\n",
        "    W,b =gradient_descent(x_trainFlat, Y_actual, y_train, iteration=100, learning_rate=0.1)\n",
        "elif user_input == \"B\":\n",
        "    W,b =stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=100, learning_rate=0.1, batch_size=64)\n",
        "elif user_input == \"C\":\n",
        "    W,b = momentum_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=100, learning_rate=0.1, batch_size=64, momentum=0.9)\n",
        "elif user_input == \"D\":\n",
        "    W,b = nesterov_gradient_descent(x_trainFlat, Y_actual, y_train, iteration=100, learning_rate=0.1, momentum=0.9, batch_size=64)\n",
        "else:\n",
        "    print(\"Invalid input. Please enter A, B, C, or D.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
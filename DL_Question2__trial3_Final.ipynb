{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvinashKushwah/cs6910_assignment1/blob/main/DL_Question2__trial3_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT_oMtPmCeS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2bf6027-ab3d-4b1f-a275-bc1c83a6f959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 0 0 ... 3 0 5]\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(y_train)\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5eb7NvCAbX",
        "outputId": "9761636d-f827-4a5a-ec1a-56ece354e72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "x_trainFlat = x_train.reshape(len(x_train),784).astype('float32')/255\n",
        "print(x_trainFlat.shape)\n",
        "x_testFlat = x_test.reshape(len(x_test),784).astype('float32')/255\n",
        "print(x_trainFlat.shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuBTUkAWG_Km"
      },
      "outputs": [],
      "source": [
        "# Activation Function\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_diff(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "# def softmax(x):\n",
        "#   # x -= np.max(x,axis =1,keepdims=True)  #subtracting to avoid numerical instablity\n",
        "#   # temp = np.exp(x)\n",
        "#   # softmax_result = temp/np.sum(temp,axis=1,keepdims=True)\n",
        "#   # return softmax_result\n",
        "#   x = x.reshape((1,10))\n",
        "#   exp_x = np.exp(x)\n",
        "#   return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "80ZYFHeQIy05",
        "outputId": "b26ab15c-b2c1-4f6e-f1b6-786fc68aa4cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(28, 28)\n",
            "enter number of Hidden layers:2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Normalize the image pixel intesity within a range of 0-1\n",
        "# x_train = x_train/255\n",
        "# x_test = x_test /255\n",
        "print(y_train.shape)\n",
        "print(x_train[0].shape)\n",
        "plt.matshow(x_train[0])\n",
        "layers = int(input(\"enter number of Hidden layers:\"))\n",
        "layers = int(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FC3bkmu2bs6"
      },
      "outputs": [],
      "source": [
        "# layers = input(\"enter number of Hidden layers:\")\n",
        "# layers = int(layers)\n",
        "# numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "# W = []                            #store the Weight matrix at each layer\n",
        "# b = []                            #store the bias vector at each layer\n",
        "\n",
        "\n",
        "# for i in range(layers):                       \n",
        "#   randomNumber = np.random.randint(4,10)\n",
        "#   numberOfNeurons.append(randomNumber)\n",
        "#   if( i== 0):\n",
        "#     w_temp =np.random.rand(randomNumber,784)\n",
        "#     b_temp = np.random.rand(randomNumber)\n",
        "#     W.append(w_temp)\n",
        "#     b.append(b_temp)\n",
        "#   else:\n",
        "#     w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1])\n",
        "#     b_temp = np.random.rand(numberOfNeurons[i])\n",
        "#     W.append(w_temp);\n",
        "#     b.append(b_temp)\n",
        "\n",
        "# output_w = np.random.rand(10,numberOfNeurons[layers-1])\n",
        "# output_b = np.random.rand(10)\n",
        "# W.append(output_w)\n",
        "# b.append(output_b)\n",
        "\n",
        "# #print(len(W))\n",
        "# #print(len(b))\n",
        "# #print(W[0].shape)\n",
        "# # def randomIntialization(dimx,dimy):\n",
        "# #   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "# #   b_temp = np.random.randint(10,size=(dimx))\n",
        "# #   return W_temp, b_temp\n",
        "\n",
        "\n",
        "\n",
        "# def feedForward(input_data):\n",
        "    \n",
        "#    Y= sigmoid(np.dot(W[0],input_data) + b[0])\n",
        "#    for i in range(1,layers):\n",
        "#       Y = sigmoid(np.dot(W[i],Y)) + b[i]\n",
        "  \n",
        "#    output = sigmoid(np.dot(W[layers],Y)) + b[layers]\n",
        "#    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# output = feedForward(x_trainFlat[0])\n",
        "# outputSoftmax = softmax(output)\n",
        "# #print(output.shape)\n",
        "# print(outputSoftmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pl3IeUQQ6cC"
      },
      "outputs": [],
      "source": [
        "# def softmax1(x):\n",
        "#   x -= np.max(x,axis =1,keepdims=True)  #subtracting to avoid numerical instablity\n",
        "#   temp = np.exp(x)\n",
        "#   softmax_result = temp/np.sum(temp,axis=1,keepdims=True)\n",
        "#   return softmax_result\n",
        " # x = x.reshape((1,10))\n",
        "  # exp_x = np.exp(x)\n",
        "  # return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def softmax1(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "  \n",
        "def ReLU(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def ReLU_deriv(x):\n",
        "    return x > 0\n",
        "\n",
        "def Initalize_Wb():\n",
        "  #layers = int(input(\"enter number of Hidden layers:\"))\n",
        "  #layers = int(layers)\n",
        "  numberOfNeurons =[]               # number of neurons at each Hidden Layer\n",
        "  W = []                            #store the Weight matrix at each layer\n",
        "  b = []                            #store the bias vector at each layer\n",
        "  for i in range(layers):                       \n",
        "    randomNumber = np.random.randint(64,256)\n",
        "    numberOfNeurons.append(randomNumber)\n",
        "    if( i== 0):\n",
        "      w_temp =np.random.rand(randomNumber,784) - 0.5\n",
        "      b_temp = np.random.rand(randomNumber,1) - 0.5\n",
        "      W.append(w_temp)\n",
        "      b.append(b_temp)\n",
        "    else:\n",
        "      w_temp = np.random.rand(numberOfNeurons[i],numberOfNeurons[i-1]) - 0.5\n",
        "      b_temp = np.random.rand(numberOfNeurons[i],1) - 0.5\n",
        "      W.append(w_temp);\n",
        "      b.append(b_temp)\n",
        "\n",
        "  print(numberOfNeurons)\n",
        "  output_w = np.random.rand(10,numberOfNeurons[layers-1]) - 0.5\n",
        "  output_b = np.random.rand(10,1) - 0.5\n",
        "  W.append(output_w)\n",
        "  b.append(output_b)\n",
        "  # print(len(W))\n",
        "  # print(len(b))\n",
        "  # print(W[0].shape)\n",
        "  # print(W[1].shape)\n",
        "  # print(W[2].shape)\n",
        "  # print(b[0].shape)\n",
        "  # print(b[1].shape)\n",
        "  # print(b[2].shape)\n",
        "\n",
        "  return W,b\n",
        "\n",
        "\n",
        "# def randomIntialization(dimx,dimy):\n",
        "#   W_temp = np.random.randint(10,size=(dimx,dimy)) # assigning random number between 0-9\n",
        "#   b_temp = np.random.randint(10,size=(dimx))\n",
        "#   return W_temp, b_temp\n",
        "\n",
        "def feedForward(input_data,W,b):\n",
        "  \n",
        "   output =[]\n",
        "   temp =[]\n",
        "   A =[]\n",
        "  #  print(\"hello\",b[0].shape)\n",
        "  #  print(b[0].reshape(-1,1).shape)\n",
        "   Y =np.dot(W[0],input_data) + b[0]\n",
        "   temp.append(Y)\n",
        "   Y= ReLU(Y)\n",
        "   A.append(Y)\n",
        "   for i in range(1,layers):\n",
        "      Y =np.dot(W[i],Y) + b[i]\n",
        "      temp.append(Y)\n",
        "      Y=ReLU(Y)\n",
        "      A.append(Y)\n",
        "   temp2=np.dot(W[layers],Y) + b[layers]\n",
        "   temp.append(temp2)\n",
        "   output=(softmax1(ReLU(temp2)))\n",
        "   #print(\"softmaxt\",output[:,1])\n",
        "   return output,temp,A\n",
        "\n",
        "\n",
        "\n",
        "#print(x_trainFlat.shape)\n",
        "#Y_predicted,Intermediate_Activation,A= feedForward(x_trainFlat.T)\n",
        "#Y_predicted = np.array(Y_predicted)\n",
        "#Intermediate_Activation = np.array(Intermediate_Activation)\n",
        "#print(Y_predicted[0][1])\n",
        "# print(Y_predicted.shape)\n",
        "# print(Intermediate_Activation[0].shape)\n",
        "# print(Intermediate_Activation[1].shape)\n",
        "# print(Intermediate_Activation[2].shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmRsw01N1gS",
        "outputId": "89c25b70-c9cc-4bfd-dd26-000d857ec4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 60000)\n"
          ]
        }
      ],
      "source": [
        "def find_Actuall_Y():\n",
        "  X,Y = x_trainFlat.shape\n",
        "  # print(X)\n",
        "  Y_actual=[]\n",
        "  for i in range(X):\n",
        "    tempX =x_trainFlat[i]\n",
        "    tempY = y_train[i]\n",
        "    vector = np.zeros(10)             # creating a vector of size 10 and assigning '1' to the correct index of trainFlat[i]\n",
        "    vector[tempY] =1\n",
        "    Y_actual.append(vector)\n",
        "    # #outputY=softmax(feedForward(x_trainFlat[i]))\n",
        "    # Y_predicted.append(outputY)\n",
        "    # loss = -np.mean(np.sum(Y_actual * np.log(Y_predicted),axis =1))          #crossEntropy loss Caluclation\n",
        "  return Y_actual\n",
        "\n",
        "\n",
        "Y_actual = find_Actuall_Y()\n",
        "Y_actual = np.array(Y_actual)\n",
        "Y_actual = Y_actual.T\n",
        "print(Y_actual.shape)\n",
        "\n",
        "# def errorCalculation(Y_actual,Y_predicted):\n",
        "#   Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7) # you may want to clip them to avoid log 0. avoiding numercial instablity\n",
        "#   temp = Y_actual.shape[0]\n",
        "#   loss = -1/temp * np.sum(np.multiply(Y_actual, np.log(Y_predicted)) + np.multiply((1 - Y_actual), np.log(1 - Y_predicted)))\n",
        "\n",
        "\n",
        "# errorLoss = errorCalculation(Y_actual, Y_predicted)\n",
        "# print(errorLoss)\n",
        "\n",
        "# Assuming Y_predicted and Y_actual are 10x60000 arrays\n",
        "def cross_entropy_loss(Y_actual, Y_predicted):\n",
        "    # Clip predicted values to avoid log(0) numerical instability\n",
        "    Y_predicted = np.clip(Y_predicted, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Compute cross-entropy loss\n",
        "    N = Y_actual.shape[1] # number of samples\n",
        "    loss = -1/N * np.sum(Y_actual * np.log(Y_predicted))\n",
        "    return loss\n",
        "\n",
        "# error = cross_entropy_loss(Y_actual, Y_predicted)\n",
        "# print(error)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcBVLnv5ftzg"
      },
      "outputs": [],
      "source": [
        "#Backpropogation Function\n",
        "\n",
        "def backpropagation(X, Y_actual, Y_predicted, Intermediate_Activation,A,W,b):\n",
        "    dW = [None] * (len(W)) # gradient of weight matrix at each layer\n",
        "    db = [None] * (len(b)) # gradient of bias vector at each layer\n",
        "    \n",
        "    # Compute derivative of loss with respect to output\n",
        "    dL_dZ = Y_predicted - Y_actual\n",
        "    \n",
        "    # Compute gradients for output layer\n",
        "   #dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[-1].T)\n",
        "    dW[-1] = 1/X.shape[0] * np.dot(dL_dZ, A[-1].T)\n",
        "    db[-1] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "   # print(dL_dZ.shape)\n",
        "    #print(db[-1].shape)\n",
        "    \n",
        "    # Backpropagate gradients through hidden layers\n",
        "    for l in reversed(range(1, len(W)-1)):\n",
        "        dL_dY = np.dot(W[l+1].T, dL_dZ)\n",
        "        # print(dL_dY.shape)\n",
        "        # print(Intermediate_Activation[l].shape)\n",
        "        # zy= sigmoid_diff(Intermediate_Activation[l-1])\n",
        "        # print(zy.shape)\n",
        "        dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[l])\n",
        "        dW[l] = 1/X.shape[0] * np.dot(dL_dZ, Intermediate_Activation[l-1].T)\n",
        "        db[l] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "\n",
        "    \n",
        "    # Compute gradients for input layer\n",
        "    dL_dY = np.dot(W[1].T, dL_dZ)\n",
        "    dL_dZ = dL_dY * ReLU_deriv(Intermediate_Activation[0])\n",
        "    dW[0] = 1/X.shape[0] * np.dot(dL_dZ, X)\n",
        "    db[0] = 1/X.shape[0] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
        "    # print(\"insside update\",len(db))\n",
        "    # print(db[0].shape)\n",
        "    # print(db[1].shape)\n",
        "    # print(db[2].shape)\n",
        "    return dW, db\n",
        "\n",
        "# dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "# print(dW[0].shape,db[0].shape)\n",
        "# print(dW[1].shape,db[1].shape)\n",
        "# print(dW[2].shape,db[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Demu45MGTSNp"
      },
      "outputs": [],
      "source": [
        "def Update(W,b,dW,db,learningRate):\n",
        "\n",
        "  for i in range (len(W)):\n",
        "    W[i] = W[i] -learningRate*dW[i]\n",
        "   # print(\"before update\",b[i].shape,db[i].shape)\n",
        "    b[i] = b[i] -learningRate* db[i]\n",
        "    #print(\"after update\",b[i].shape, W[i].shape)\n",
        "  \n",
        "  return W,b\n",
        "\n",
        "\n",
        "#W,b=Update(W,b,dW,db,0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaLQM-cQelX4"
      },
      "outputs": [],
      "source": [
        " \n",
        "#  get_prediction(Y_predicted,y_train):\n",
        "\n",
        "\n",
        "\n",
        "# def getAccuracy()\n",
        "\n",
        "\n",
        "\n",
        "# def gradient_descent(x_trainFlat,y_train,Y_actual,iteration,learning_rate,W,b):\n",
        "\n",
        "#   for i in range(iteration):\n",
        "#     Y_predicted,Intermediate_Activation,A= feedForward(x_trainFlat.T)\n",
        "#     Y_predicted = np.array(Y_predicted)\n",
        "#     dW,db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation)\n",
        "#     W,b = Update(W,b,dW,db,learning_rate)\n",
        "#     if(i%50 ==0):\n",
        "#       print(\"iteration:\",i)\n",
        "#       print(\"Accuracy:\",get_Accuracy(get_prediction(Y_predicted),y_train))\n",
        "    \n",
        "#     return W,b\n",
        "\n",
        "\n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat,Y_actual,1000,0.1,W,b)\n",
        "\n",
        "\n",
        "# def getAccuracy(y_predicted, y_actual):\n",
        "#     \"\"\"\n",
        "#     Compute the accuracy of the predicted labels y_predicted compared to the actual labels y_actual.\n",
        "#     \"\"\"\n",
        "#     y_pred_labels = np.argmax(y_predicted, axis=0)\n",
        "#     print(\"Ypredicted\",y_pred_labels.shape)\n",
        "#     print(\"y_actual\",Y_actual[:,1])\n",
        "#     Intersection = y_pred_labels * Y_actual\n",
        "#     matches = np.all(Intersection == 1, axis=0)\n",
        "#     accuracy = np.mean(matches)\n",
        "    \n",
        "#     return accuracy\n",
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "# def get_prediction(Y_predicted):\n",
        "#     \"\"\"\n",
        "#     Convert the predicted output of the network Y_predicted to a one-hot encoded array of labels.\n",
        "#     \"\"\"\n",
        "#     y_pred_labels = np.argmax(Y_predicted, axis=0)\n",
        "#     n_labels = Y_predicted.shape[0]\n",
        "#     n_examples = Y_predicted.shape[1]\n",
        "#     y_pred = np.zeros((n_labels, n_examples))\n",
        "#     y_pred[y_pred_labels, np.arange(n_examples)] = 1\n",
        "#   return y_pred\n",
        "\n",
        "# def gradient_descent(x_trainFlat,Y_actual,y_train,iteration,learning_rate):\n",
        "#     W,b = Initalize_Wb()\n",
        "#     print(len(W),len(b))\n",
        "#     for i in range(iteration):\n",
        "#         Y_predicted, Intermediate_Activation, A = feedForward(x_trainFlat.T,W,b)\n",
        "#         dW, db = backpropagation(x_trainFlat,Y_actual,Y_predicted,Intermediate_Activation,A,W,b)\n",
        "#         # print(\"x\",len(b),b[0].shape,b[1].shape,b[2].shape)\n",
        "#         W,b = Update(W,b,dW,db,learning_rate)\n",
        "#         #print(\"len of b\",len(b))\n",
        "#         if i % 10 == 0:\n",
        "#             print(\"iteration:\", i)\n",
        "#            # y_pred = get_prediction(Y_predicted)\n",
        "#             #print(\"Ypredicted\",Y_predicted[:,1])\n",
        "#             #print(\"y_actual\",Y_actual[:,1])\n",
        "#             #acuracy = getAccuracy(Y_predicted, Y_actual)\n",
        "#             predictions = get_predictions(Y_predicted)\n",
        "#             print(\"Accuracy:\", get_accuracy(predictions,y_train)*100)\n",
        "#     return W,b\n",
        "     \n",
        "\n",
        "# W,b = gradient_descent(x_trainFlat, Y_actual,y_train,1000, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, batch_size, epochs, learning_rate):\n",
        "#     W, b = Initalize_Wb()\n",
        "#     print(len(W), len(b))\n",
        "#     m = x_trainFlat.shape[1]\n",
        "#     print(m)\n",
        "#     for epoch in range(epochs):\n",
        "#         epoch_loss = 0\n",
        "#         for i in range(0, m, batch_size):\n",
        "#             x_batch = x_trainFlat[:, i:i+batch_size]\n",
        "#             y_batch = Y_actual[:, i:i+batch_size]\n",
        "#             Y_predicted, Intermediate_Activation, A = feedForward(x_batch.T, W, b)\n",
        "#             dW, db = backpropagation(x_batch, y_batch, Y_predicted, Intermediate_Activation, A, W, b)\n",
        "#             W, b = Update(W, b, dW, db, learning_rate)\n",
        "#             epoch_loss += np.sum((Y_predicted - y_batch) ** 2)\n",
        "#         epoch_loss /= m\n",
        "#         if epoch % 10 == 0:\n",
        "#             print(f\"Epoch {epoch}, Loss {epoch_loss}\")\n",
        "#             predictions = get_predictions(Y_predicted)\n",
        "#             print(\"Accuracy:\", get_accuracy(predictions,y_train)*100)\n",
        "#     return W, b\n",
        "\n",
        "# W, b = stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, batch_size=64, epochs=100, learning_rate=0.1)\n"
      ],
      "metadata": {
        "id": "jsDOkq_MX4Ar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "9c029f06-485c-4d3a-8aef-034a5b2d377c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104, 181]\n",
            "3 3\n",
            "784\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8a3a0c251bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainFlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-8a3a0c251bf7>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(x_trainFlat, Y_actual, y_train, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_trainFlat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_actual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mY_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntermediate_Activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntermediate_Activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-23ab0ac8656b>\u001b[0m in \u001b[0;36mfeedForward\u001b[0;34m(input_data, W, b)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;31m#  print(\"hello\",b[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;31m#  print(b[0].reshape(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m    \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m    \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m    \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (104,784) and (64,60000) not aligned: 784 (dim 1) != 64 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def stochastic_gradient_descent(x_trainFlat, Y_actual, y_train, iteration, learning_rate, batch_size):\n",
        "    W, b = Initalize_Wb()\n",
        "    for i in range(iteration):\n",
        "        # Randomly shuffle the training data and split it into batches\n",
        "        idx = np.random.permutation(len(x_trainFlat))\n",
        "        x_trainFlat_shuffled = x_trainFlat[idx]\n",
        "        Y_actual_shuffled = Y_actual[:, idx]\n",
        "        y_train_shuffled = np.array(y_train)[idx]\n",
        "        for j in range(0, len(x_trainFlat), batch_size):\n",
        "            # Select a batch of training examples\n",
        "            x_batch = x_trainFlat_shuffled[j:j+batch_size]\n",
        "            Y_actual_batch = Y_actual_shuffled[:, j:j+batch_size]\n",
        "            y_train_batch = y_train_shuffled[j:j+batch_size]\n",
        "            # Compute the forward pass and backpropagation for the batch\n",
        "            Y_predicted_batch, Intermediate_Activation_batch, A_batch = feedForward(x_batch.T, W, b)\n",
        "            dW_batch, db_batch = backpropagation(x_batch, Y_actual_batch, Y_predicted_batch, Intermediate_Activation_batch, A_batch, W, b)\n",
        "            # Update the parameters using the batch gradient\n",
        "            W, b = Update(W, b, dW_batch, db_batch, learning_rate)\n",
        "        if i % 10 == 0:\n",
        "            # Compute the accuracy on the entire training set\n",
        "            Y_predicted, _, _ = feedForward(x_trainFlat.T, W, b)\n",
        "            predictions = get_predictions(Y_predicted)\n",
        "            accuracy = get_accuracy(predictions, y_train) * 100\n",
        "            print(f\"Iteration {i}: accuracy = {accuracy:.2f}%\")\n",
        "    return W, b\n",
        "\n",
        "\n",
        "W,b = stochastic_gradient_descent(x_trainFlat,Y_actual,y_train,100,0.1,64)"
      ],
      "metadata": {
        "id": "6hQ5LsDPX5AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c0aab7-65cc-4ec3-e855-04c88e5f21c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[157, 90]\n",
            "[9 0 2 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 0: accuracy = 77.25%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 10: accuracy = 87.73%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 20: accuracy = 90.21%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 30: accuracy = 91.97%\n",
            "[9 0 3 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 40: accuracy = 92.94%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 50: accuracy = 92.98%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 60: accuracy = 94.22%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 70: accuracy = 94.91%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 80: accuracy = 94.94%\n",
            "[9 0 0 ... 3 0 5] [9 0 0 ... 3 0 5]\n",
            "Iteration 90: accuracy = 94.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10RI4yyqpitB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}